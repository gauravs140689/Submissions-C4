{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e9123a",
   "metadata": {},
   "source": [
    "# RAG Implementation with LlamaIndex and LanceDB\n",
    "\n",
    "This notebook demonstrates a complete RAG (Retrieval Augmented Generation) implementation using LlamaIndex and LanceDB. We'll explore three different approaches:\n",
    "\n",
    "1. **Vector Search Only** - Fast retrieval without LLM generation\n",
    "2. **HuggingFace API Integration** - Cloud-based LLM with authentication\n",
    "3. **Local LLM with Ollama** - Complete local solution\n",
    "\n",
    "## Overview\n",
    "\n",
    "The notebook covers:\n",
    "- Data loading and preparation from HuggingFace datasets\n",
    "- Vector store setup with LanceDB\n",
    "- Embedding generation with HuggingFace models\n",
    "- Three different query approaches with increasing complexity\n",
    "- Utility functions for table exploration and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf9504",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dc3a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install all required packages\n",
    "# !pip install llama-index llama-index-vector-stores-lancedb llama-index-embeddings-huggingface llama-index-llms-huggingface-api lancedb datasets -q\n",
    "\n",
    "# # Additional packages for local LLM and utilities\n",
    "# !pip install llama-index-llms-ollama requests -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae153d74",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b51b6153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lancedb\n",
    "import subprocess\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "# LlamaIndex core components\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# Embedding and vector store\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "\n",
    "# LLM integrations\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Async support for notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(\"All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6297a29",
   "metadata": {},
   "source": [
    "## 3. Data Preparation and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1df8ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 100 personas from dataset...\n",
      "Prepared 100 documents\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(num_samples=100):\n",
    "    \"\"\"\n",
    "    Load dataset and create document files\n",
    "    \"\"\"\n",
    "    print(f\"Loading {num_samples} personas from dataset...\")\n",
    "    \n",
    "    # Load the personas dataset\n",
    "    dataset = load_dataset(\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
    "    \n",
    "    # Create data directory\n",
    "    Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save personas as text files and create Document objects\n",
    "    documents = []\n",
    "    for i, persona in enumerate(dataset.select(range(min(num_samples, len(dataset))))):\n",
    "        # Create Document objects for LlamaIndex\n",
    "        doc = Document(\n",
    "            text=persona[\"persona\"],\n",
    "            metadata={\n",
    "                \"persona_id\": i,\n",
    "                \"source\": \"finepersonas-dataset\"\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "        \n",
    "        # Optionally save to files as well\n",
    "        with open(Path(\"data\") / f\"persona_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(persona[\"persona\"])\n",
    "    \n",
    "    print(f\"Prepared {len(documents)} documents\")\n",
    "    return documents\n",
    "\n",
    "# Load the data\n",
    "documents = prepare_data(num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def1b98",
   "metadata": {},
   "source": [
    "## 4. LanceDB Vector Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6741ba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up LanceDB connection...\n",
      "Connected to LanceDB, table: personas_rag\n"
     ]
    }
   ],
   "source": [
    "def setup_lancedb_store(table_name=\"personas_rag\"):\n",
    "    \"\"\"\n",
    "    Initialize LanceDB and create/connect to a table\n",
    "    \"\"\"\n",
    "    print(\"Setting up LanceDB connection...\")\n",
    "    \n",
    "    # Create or connect to LanceDB\n",
    "    db = lancedb.connect(\"./lancedb_data\")\n",
    "    \n",
    "    # LlamaIndex will handle table creation with proper schema\n",
    "    print(f\"Connected to LanceDB, table: {table_name}\")\n",
    "    \n",
    "    return db, table_name\n",
    "\n",
    "# Setup database connection\n",
    "db, table_name = setup_lancedb_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d2ce0",
   "metadata": {},
   "source": [
    "## 5. Vector Embeddings and Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ec8a431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding model and ingestion pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07847df4ed9f4393954d614d5521ee23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: BAAI/bge-small-en-v1.5\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents and creating embeddings...\n",
      "Successfully processed 100 text chunks\n"
     ]
    }
   ],
   "source": [
    "async def create_and_populate_index(documents, db, table_name):\n",
    "    \"\"\"\n",
    "    Create ingestion pipeline and populate LanceDB with embeddings\n",
    "    \"\"\"\n",
    "    print(\"Creating embedding model and ingestion pipeline...\")\n",
    "    \n",
    "    # Initialize embedding model\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        model_name=\"BAAI/bge-small-en-v1.5\"\n",
    "    )\n",
    "    \n",
    "    # Create LanceDB vector store\n",
    "    vector_store = LanceDBVectorStore(\n",
    "        uri=\"./lancedb_data\",\n",
    "        table_name=table_name,\n",
    "        mode=\"overwrite\"  # overwrite existing table\n",
    "    )\n",
    "    \n",
    "    # Create ingestion pipeline\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
    "            embed_model,\n",
    "        ],\n",
    "        vector_store=vector_store,\n",
    "    )\n",
    "    \n",
    "    print(\"Processing documents and creating embeddings...\")\n",
    "    # Run the pipeline to process documents and store in LanceDB\n",
    "    nodes = await pipeline.arun(documents=documents)\n",
    "    print(f\"Successfully processed {len(nodes)} text chunks\")\n",
    "    \n",
    "    return vector_store, embed_model\n",
    "\n",
    "# Create embeddings and populate vector store\n",
    "vector_store, embed_model = await create_and_populate_index(documents, db, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46affc4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stores_text': True,\n",
       " 'is_embedding_query': True,\n",
       " 'flat_metadata': True,\n",
       " 'uri': './lancedb_data',\n",
       " 'vector_column_name': 'vector',\n",
       " 'nprobes': 20,\n",
       " 'refine_factor': None,\n",
       " 'text_key': 'text',\n",
       " 'doc_id_key': 'doc_id',\n",
       " 'api_key': None,\n",
       " 'region': None,\n",
       " 'mode': 'overwrite',\n",
       " 'query_type': 'vector',\n",
       " 'overfetch_factor': 1,\n",
       " 'class_name': 'base_component'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd7550",
   "metadata": {},
   "source": [
    "## 6. Option 1: Vector Search Only (No LLM)\n",
    "\n",
    "This approach provides fast document retrieval without LLM generation. Perfect for finding relevant content quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b00c89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Vector Search (No LLM needed)\n",
      "==================================================\n",
      "\n",
      "Query: technology and artificial intelligence expert\n",
      "------------------------------\n",
      "\n",
      "Result 1 (Score: 0.589):\n",
      "A computer scientist or electronics engineer researching alternative sustainable materials for neuromorphic computing and memristor development or A science journalist covering emerging technologies a...\n",
      "\n",
      "Result 2 (Score: 0.589):\n",
      "A computer scientist or electronics engineer researching alternative sustainable materials for neuromorphic computing and memristor development or A science journalist covering emerging technologies a...\n",
      "\n",
      "Result 3 (Score: 0.626):\n",
      "An aerospace engineer or astrobiologist interested in innovative technologies for space exploration....\n",
      "\n",
      "Query: teacher educator professor\n",
      "------------------------------\n",
      "\n",
      "Result 1 (Score: 0.577):\n",
      "An English language arts teacher with a focus on upper elementary education....\n",
      "\n",
      "Result 2 (Score: 0.577):\n",
      "An English language arts teacher with a focus on upper elementary education....\n",
      "\n",
      "Result 3 (Score: 0.584):\n",
      "An educator, possibly a middle school geography teacher or an NCERT textbook author, focused on creating educational content for Class 7 students studying geography, with an emphasis on clear explanat...\n",
      "\n",
      "Query: environment climate sustainability\n",
      "------------------------------\n",
      "\n",
      "Result 1 (Score: 0.683):\n",
      "An environmental scientist focused on climate change and pollution issues, or a sustainability advocate pushing for global action on reducing greenhouse gas emissions....\n",
      "\n",
      "Result 2 (Score: 0.683):\n",
      "An environmental scientist focused on climate change and pollution issues, or a sustainability advocate pushing for global action on reducing greenhouse gas emissions....\n",
      "\n",
      "Result 3 (Score: 0.723):\n",
      "An environmental policy analyst focused on sustainable transportation alternatives, or an automotive engineer with a specialization in electric vehicles, likely wrote this text, aiming to inform the g...\n",
      "\n",
      "Query: art culture heritage creative\n",
      "------------------------------\n",
      "\n",
      "Result 1 (Score: 0.625):\n",
      "A local art historian and museum professional interested in 19th-century American art and the local cultural heritage of Cincinnati....\n",
      "\n",
      "Result 2 (Score: 0.625):\n",
      "A local art historian and museum professional interested in 19th-century American art and the local cultural heritage of Cincinnati....\n",
      "\n",
      "Result 3 (Score: 0.686):\n",
      "An art historian or scholar of classical Chinese culture, particularly one fascinated by the intersection of nature, spirituality, and art in ancient Chinese civilization....\n"
     ]
    }
   ],
   "source": [
    "def perform_vector_search(db, table_name, query_text, embed_model, top_k=5):\n",
    "    \"\"\"\n",
    "    Perform direct vector search on LanceDB\n",
    "    \"\"\"\n",
    "    # Get query embedding\n",
    "    query_embedding = embed_model.get_text_embedding(query_text)\n",
    "    \n",
    "    # Open table and perform search\n",
    "    table = db.open_table(table_name)\n",
    "    results = table.search(query_embedding).limit(top_k).to_pandas()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_vector_search():\n",
    "    \"\"\"\n",
    "    Test vector search functionality with sample queries\n",
    "    \"\"\"\n",
    "    print(\"Testing Vector Search (No LLM needed)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test queries\n",
    "    queries = [\n",
    "        \"technology and artificial intelligence expert\",\n",
    "        \"teacher educator professor\",\n",
    "        \"environment climate sustainability\", \n",
    "        \"art culture heritage creative\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Perform search\n",
    "        results = perform_vector_search(db, table_name, query, embed_model, top_k=3)\n",
    "        \n",
    "        for idx, row in results.iterrows():\n",
    "            score = row.get('_distance', 'N/A')\n",
    "            text = row.get('text', 'N/A')\n",
    "            \n",
    "            # Format score\n",
    "            if isinstance(score, (int, float)):\n",
    "                score_str = f\"{score:.3f}\"\n",
    "            else:\n",
    "                score_str = str(score)\n",
    "            \n",
    "            print(f\"\\nResult {idx + 1} (Score: {score_str}):\")\n",
    "            print(f\"{text[:200]}...\")\n",
    "\n",
    "# Run vector search test\n",
    "test_vector_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7668d",
   "metadata": {},
   "source": [
    "## 7. Option 2: RAG with HuggingFace API\n",
    "\n",
    "This approach uses HuggingFace's cloud API for LLM generation. Requires API token authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b797dda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG with HuggingFace API\n",
      "========================================\n",
      "\n",
      "Query: Find personas interested in technology and AI\n",
      "------------------------------\n",
      "Response: Individuals with a background in aerospace engineering or astrobiology and a focus on innovative technologies for space exploration are likely to have an interest in technology and AI.\n",
      "\n",
      "Query: Who are the educators or teachers in the dataset?\n",
      "------------------------------\n",
      "Response: An English language arts teacher with a focus on upper elementary education.\n",
      "\n",
      "Query: Describe personas working with environmental topics\n",
      "------------------------------\n",
      "Response: Individuals who focus on climate change and pollution issues, or those advocating for global action to reduce greenhouse gas emissions, are likely to be working with environmental topics.\n"
     ]
    }
   ],
   "source": [
    "# Set your HuggingFace API token here\n",
    "# Get your free token from: https://huggingface.co/settings/tokens\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = \"\"  # Replace with your actual token\n",
    "\n",
    "def create_query_engine(vector_store, embed_model, llm=None):\n",
    "    \"\"\"\n",
    "    Create a query engine from the vector store\n",
    "    \"\"\"\n",
    "    # Create index from vector store\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store,\n",
    "        embed_model=embed_model\n",
    "    )\n",
    "    \n",
    "    # Setup LLM if provided\n",
    "    query_engine_kwargs = {}\n",
    "    if llm:\n",
    "        query_engine_kwargs['llm'] = llm\n",
    "    \n",
    "    # Create query engine\n",
    "    query_engine = index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        **query_engine_kwargs\n",
    "    )\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "def query_rag(query_engine, question):\n",
    "    \"\"\"\n",
    "    Query the RAG system and return response\n",
    "    \"\"\"\n",
    "    response = query_engine.query(question)\n",
    "    return response\n",
    "\n",
    "async def test_huggingface_rag():\n",
    "    \"\"\"\n",
    "    Test RAG with HuggingFace API\n",
    "    \"\"\"\n",
    "    print(\"Testing RAG with HuggingFace API\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Initialize HuggingFace LLM with authentication\n",
    "        llm = HuggingFaceInferenceAPI(\n",
    "            model_name=  \"meta-llama/Meta-Llama-3-8B-Instruct\", #\"HuggingFaceH4/zephyr-7b-beta\", #\"HuggingFaceTB/SmolLM3-3B\",\n",
    "            token=os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
    "        )\n",
    "        \n",
    "        # Create query engine\n",
    "        query_engine = create_query_engine(vector_store, embed_model, llm)\n",
    "        \n",
    "        # Test queries\n",
    "        queries = [\n",
    "            \"Find personas interested in technology and AI\",\n",
    "            \"Who are the educators or teachers in the dataset?\",\n",
    "            \"Describe personas working with environmental topics\"\n",
    "        ]\n",
    "        \n",
    "        for query in queries:\n",
    "            print(f\"\\nQuery: {query}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            try:\n",
    "                response = query_rag(query_engine, query)\n",
    "                print(f\"Response: {response}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Setup error: {e}\")\n",
    "        print(\"Make sure to set your HuggingFace API token above\")\n",
    "\n",
    "# Uncomment the line below after setting your API token\n",
    "await test_huggingface_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c437ab16",
   "metadata": {},
   "source": [
    "## 8. Option 3: RAG with Local LLM (Ollama)\n",
    "\n",
    "This approach uses a completely local LLM setup. No internet required after initial setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b3744e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is installed: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pull_ollama_model(model_name=\"llama3\"):\n",
    "    \"\"\"Pull a lightweight model for local inference\"\"\"\n",
    "    try:\n",
    "        # Use 'llama3' as the default model name for broad compatibility\n",
    "        print(f\"Pulling model: {model_name}\")\n",
    "        result = subprocess.run([\"ollama\", \"pull\", model_name], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"Model {model_name} pulled successfully!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to pull model: {result.stderr}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error pulling model: {e}\")\n",
    "        return False\n",
    "\n",
    "def setup_ollama():\n",
    "    \"\"\"Complete Ollama setup\"\"\"\n",
    "    if not check_ollama_installed():\n",
    "        print(\"Ollama needs to be installed.\")\n",
    "        download_ollama()\n",
    "        return False\n",
    "    \n",
    "    if not start_ollama_service():\n",
    "        return False\n",
    "    \n",
    "    if not pull_ollama_model(\"llama3\"):\n",
    "        return False\n",
    "    \n",
    "    print(\"Ollama setup complete!\")\n",
    "    return True\n",
    "\n",
    "# Check Ollama installation\n",
    "check_ollama_installed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d2011bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG with Local LLM (Ollama)\n",
      "========================================\n",
      "\n",
      "Query: Find personas interested in technology and AI\n",
      "------------------------------\n",
      "Response: Persona 89.\n",
      "\n",
      "Query: Who are the educators or teachers in the dataset?\n",
      "------------------------------\n",
      "Response: English language arts teachers.\n",
      "\n",
      "Query: Describe personas working with environmental topics\n",
      "------------------------------\n",
      "Response: A dedicated professional striving to mitigate the devastating effects of climate change and pollution, as well as a passionate advocate relentlessly pursuing global efforts to curtail greenhouse gas emissions. These individuals are deeply committed to preserving our planet's delicate ecological balance, recognizing the urgent need for collective action in combating environmental degradation.\n"
     ]
    }
   ],
   "source": [
    "async def test_local_llm_rag():\n",
    "    \"\"\"\n",
    "    Test RAG with local Ollama LLM\n",
    "    \"\"\"\n",
    "    print(\"Testing RAG with Local LLM (Ollama)\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Initialize local Ollama LLM\n",
    "        llm = Ollama(\n",
    "            model=\"llama3\",\n",
    "            base_url=\"http://localhost:11434\",\n",
    "            request_timeout=60.0\n",
    "        )\n",
    "        \n",
    "        # Create query engine\n",
    "        query_engine = create_query_engine(vector_store, embed_model, llm)\n",
    "        \n",
    "        # Test queries\n",
    "        queries = [\n",
    "            \"Find personas interested in technology and AI\",\n",
    "            \"Who are the educators or teachers in the dataset?\",\n",
    "            \"Describe personas working with environmental topics\"\n",
    "        ]\n",
    "        \n",
    "        for query in queries:\n",
    "            print(f\"\\nQuery: {query}\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            try:\n",
    "                response = query_rag(query_engine, query)\n",
    "                print(f\"Response: {response}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                print(\"Make sure Ollama is running with: ollama serve\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Setup error: {e}\")\n",
    "        print(\"Make sure Ollama is installed and running\")\n",
    "\n",
    "# Uncomment after Ollama setup is complete\n",
    "await test_local_llm_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c42b76",
   "metadata": {},
   "source": [
    "## 9. Utility Functions and Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a81e5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage Examples:\n",
      "==============================\n",
      "\n",
      "1. Vector Search Only:\n",
      "   test_vector_search()\n",
      "\n",
      "2. HuggingFace API RAG:\n",
      "   # Set API token first\n",
      "   os.environ['HUGGINGFACE_API_KEY'] = 'your_token'\n",
      "   await test_huggingface_rag()\n",
      "\n",
      "3. Local LLM RAG:\n",
      "   # Install and setup Ollama first\n",
      "   setup_ollama()\n",
      "   await test_local_llm_rag()\n",
      "\n",
      "4. Explore Database:\n",
      "   explore_lancedb_table(db, table_name)\n"
     ]
    }
   ],
   "source": [
    "def explore_lancedb_table(db, table_name):\n",
    "    \"\"\"\n",
    "    Explore the structure and content of the LanceDB table\n",
    "    \"\"\"\n",
    "    try:\n",
    "        table = db.open_table(table_name)\n",
    "        \n",
    "        print(\"Table Schema:\")\n",
    "        print(table.schema)\n",
    "        \n",
    "        print(f\"\\nTotal records: {table.count_rows()}\")\n",
    "        \n",
    "        print(\"\\nSample records:\")\n",
    "        df = table.to_pandas().head()\n",
    "        print(df)\n",
    "        \n",
    "        return table\n",
    "    except Exception as e:\n",
    "        print(f\"Error exploring table: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_filtered_query_engine(db, table_name, embed_model, filter_dict=None):\n",
    "    \"\"\"\n",
    "    Create a query engine with metadata filtering capabilities\n",
    "    \"\"\"\n",
    "    from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator\n",
    "    \n",
    "    # Reconnect to existing table\n",
    "    vector_store = LanceDBVectorStore(\n",
    "        uri=\"./lancedb_data\",\n",
    "        table_name=table_name,\n",
    "        mode=\"read\"\n",
    "    )\n",
    "    \n",
    "    # Create index\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store=vector_store,\n",
    "        embed_model=embed_model\n",
    "    )\n",
    "    \n",
    "    # Create query engine with filters if provided\n",
    "    if filter_dict:\n",
    "        filters = MetadataFilters(\n",
    "            filters=[\n",
    "                MetadataFilter(\n",
    "                    key=key,\n",
    "                    value=value,\n",
    "                    operator=FilterOperator.EQ\n",
    "                ) for key, value in filter_dict.items()\n",
    "            ]\n",
    "        )\n",
    "        query_engine = index.as_query_engine(\n",
    "            filters=filters,\n",
    "            response_mode=\"tree_summarize\"\n",
    "        )\n",
    "    else:\n",
    "        query_engine = index.as_query_engine(\n",
    "            response_mode=\"tree_summarize\"\n",
    "        )\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "async def batch_process_documents(documents, batch_size=50):\n",
    "    \"\"\"\n",
    "    Process documents in batches for large datasets\n",
    "    \"\"\"\n",
    "    embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    \n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        table_name = f\"personas_batch_{i//batch_size}\"\n",
    "        \n",
    "        vector_store = LanceDBVectorStore(\n",
    "            uri=\"./lancedb_data\",\n",
    "            table_name=table_name,\n",
    "            mode=\"overwrite\"\n",
    "        )\n",
    "        \n",
    "        pipeline = IngestionPipeline(\n",
    "            transformations=[\n",
    "                SentenceSplitter(chunk_size=512, chunk_overlap=20),\n",
    "                embed_model,\n",
    "            ],\n",
    "            vector_store=vector_store,\n",
    "        )\n",
    "        \n",
    "        nodes = await pipeline.arun(documents=batch)\n",
    "        print(f\"Processed batch {i//batch_size + 1}: {len(nodes)} nodes\")\n",
    "\n",
    "def show_usage_examples():\n",
    "    \"\"\"\n",
    "    Display usage examples for different scenarios\n",
    "    \"\"\"\n",
    "    print(\"Usage Examples:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    print(\"\\n1. Vector Search Only:\")\n",
    "    print(\"   test_vector_search()\")\n",
    "    \n",
    "    print(\"\\n2. HuggingFace API RAG:\")\n",
    "    print(\"   # Set API token first\")\n",
    "    print(\"   os.environ['HUGGINGFACE_API_KEY'] = 'your_token'\")\n",
    "    print(\"   await test_huggingface_rag()\")\n",
    "    \n",
    "    print(\"\\n3. Local LLM RAG:\")\n",
    "    print(\"   # Install and setup Ollama first\")\n",
    "    print(\"   setup_ollama()\")\n",
    "    print(\"   await test_local_llm_rag()\")\n",
    "    \n",
    "    print(\"\\n4. Explore Database:\")\n",
    "    print(\"   explore_lancedb_table(db, table_name)\")\n",
    "\n",
    "# Show usage examples\n",
    "show_usage_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef63a577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Schema:\n",
      "id: string\n",
      "doc_id: string\n",
      "vector: fixed_size_list<item: float>[384]\n",
      "  child 0, item: float\n",
      "text: string\n",
      "metadata: struct<_node_content: string, _node_type: string, doc_id: string, document_id: string, persona_id: i (... 41 chars omitted)\n",
      "  child 0, _node_content: string\n",
      "  child 1, _node_type: string\n",
      "  child 2, doc_id: string\n",
      "  child 3, document_id: string\n",
      "  child 4, persona_id: int64\n",
      "  child 5, ref_doc_id: string\n",
      "  child 6, source: string\n",
      "\n",
      "Total records: 200\n",
      "\n",
      "Sample records:\n",
      "                                     id                                doc_id  \\\n",
      "0  44d962f6-eaa5-4f38-9634-c23e4649d20a  3a6c27f6-f2f1-41b2-bf13-d927254dcf5a   \n",
      "1  08f9d013-16d7-43de-94e8-3ff85938f6b6  b619139e-b58c-4707-a5fe-cc56978e8483   \n",
      "2  74dad8ce-f8c3-4bd2-b32d-a57f5f812ace  73b147c6-b6b3-4874-b9fa-8f2ee758b378   \n",
      "3  af6fe3da-be6c-4bf1-b5bd-6d5a73df3a78  fb7145dd-1ad3-4175-95f9-740fb37fa6f3   \n",
      "4  200e9712-c038-4805-802d-0cdfc1fa500d  5c8edce5-d282-4c6c-888c-95afc48ba5db   \n",
      "\n",
      "                                              vector  \\\n",
      "0  [0.009862666, 0.036334753, 0.031320974, 0.0116...   \n",
      "1  [0.024929097, 0.073380455, 0.027212614, -0.027...   \n",
      "2  [-0.04570135, 0.063290395, 0.037031356, -0.034...   \n",
      "3  [-0.011168339, 0.066961005, 0.02281371, -0.029...   \n",
      "4  [-0.04400234, 0.02936609, 0.0010741384, -0.038...   \n",
      "\n",
      "                                                text  \\\n",
      "0  A local art historian and museum professional ...   \n",
      "1  An anthropologist or a cultural expert interes...   \n",
      "2  A military historian specializing in Japanese ...   \n",
      "3  An electrical engineering student or novice el...   \n",
      "4  A law professor or academic, likely with exper...   \n",
      "\n",
      "                                            metadata  \n",
      "0  {'_node_content': '{\"id_\": \"44d962f6-eaa5-4f38...  \n",
      "1  {'_node_content': '{\"id_\": \"08f9d013-16d7-43de...  \n",
      "2  {'_node_content': '{\"id_\": \"74dad8ce-f8c3-4bd2...  \n",
      "3  {'_node_content': '{\"id_\": \"af6fe3da-be6c-4bf1...  \n",
      "4  {'_node_content': '{\"id_\": \"200e9712-c038-4805...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LanceTable(name='personas_rag', version=2, _conn=LanceDBConnection(uri='/Users/chandru/python-sandbox/Outskill_AI_Acc_Assignments/RAG/lancedb_data'))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_lancedb_table(db, table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60273d9b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides three complete RAG implementation approaches:\n",
    "\n",
    "### Option 1: Vector Search Only\n",
    "- **Best for**: Fast document retrieval, no generation needed\n",
    "- **Advantages**: Very fast, no API costs, no setup complexity\n",
    "- **Use case**: Finding relevant documents, initial exploration\n",
    "\n",
    "### Option 2: HuggingFace API\n",
    "- **Best for**: High-quality responses with cloud LLMs\n",
    "- **Advantages**: Latest models, no local resources needed\n",
    "- **Requirements**: HuggingFace API token\n",
    "- **Use case**: Production applications with budget for API calls\n",
    "\n",
    "### Option 3: Local LLM (Ollama)\n",
    "- **Best for**: Complete privacy, no internet dependency\n",
    "- **Advantages**: No API costs, full control, offline capability\n",
    "- **Requirements**: Ollama installation, local compute resources\n",
    "- **Use case**: Private data, cost-sensitive applications\n",
    "\n",
    "Choose the approach that best fits your needs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
