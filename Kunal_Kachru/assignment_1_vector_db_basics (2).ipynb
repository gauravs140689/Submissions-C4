{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQgv5Mzj0h4V"
      },
      "source": [
        "# Assignment 1: Vector Database Creation and Retrieval\n",
        "## Day 6 Session 2 - RAG Fundamentals\n",
        "\n",
        "**OBJECTIVE:** Create a vector database from a folder of documents and implement basic retrieval functionality.\n",
        "\n",
        "**LEARNING GOALS:**\n",
        "- Understand document loading with SimpleDirectoryReader\n",
        "- Learn vector store setup with LanceDB\n",
        "- Implement vector index creation\n",
        "- Perform semantic search and retrieval\n",
        "\n",
        "**DATASET:** Use the data folder in `Day_6/session_2/data/` which contains multiple file types\n",
        "\n",
        "**INSTRUCTIONS:**\n",
        "1. Complete each function by replacing the TODO comments with actual implementation\n",
        "2. Run each cell after completing the function to test it\n",
        "3. The answers can be found in the existing notebooks in the `llamaindex_rag/` folder\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-xy2L0Z01LI",
        "outputId": "d7c67b4c-ea6a-46f0-a2d0-1321d5d663c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r \"/content/drive/MyDrive/ColabNotebooks/requirements.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhlK9LxZ37a6",
        "outputId": "525a2bed-ff56-4195-b47a-c50f31c432fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: huggingface-hub 1.3.7 does not provide the extra 'inference'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mXA0O3v0h4Y",
        "outputId": "cdb89a2f-f0e5-4f7d-b772-39c893c0fa2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "5b8b2b76a13848668080f0bcf791e5f1",
            "d351575db0f14dbeb63cacd1a7baac4b",
            "879409bd67fa4896bbec7570735bdb0a",
            "719eff8d8b0441dfadf1dd936c793637",
            "83bea923e4f7404d9eedfe486b688a3e",
            "07e347f328504c91b91d2edf5cc11c5f",
            "160e206f2ade458d942ded44263ec712",
            "ae915866cb8545488890c6629d1012b2",
            "b5a5e097e8c94dcabf6b114e90456a1b",
            "70610deda338473598c905f6648ab8d8",
            "b34bf5cdcc344a10b88df63cebcf6973"
          ]
        },
        "id": "q8C9EBNJ0h4Z",
        "outputId": "3781381b-d6e5-4f8e-da71-691f3555416f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… OPENROUTER_API_KEY found - using OpenRouter for LLM operations\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b8b2b76a13848668080f0bcf791e5f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: BAAI/bge-small-en-v1.5\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LlamaIndex configured with local embeddings\n",
            "   Using BAAI/bge-small-en-v1.5 for document embeddings\n"
          ]
        }
      ],
      "source": [
        "# Configure LlamaIndex Settings (Using OpenRouter - No OpenAI API Key needed)\n",
        "from google.colab import userdata\n",
        "def setup_llamaindex_settings():\n",
        "    \"\"\"\n",
        "    Configure LlamaIndex with local embeddings and OpenRouter for LLM.\n",
        "    This assignment focuses on vector database operations, so we'll use local embeddings only.\n",
        "    \"\"\"\n",
        "    # Check for OpenRouter API key (for future use, not needed for this basic assignment)\n",
        "    # small tweak to get OpenRouter API key\n",
        "    api_key_value = userdata.get('OPENROUTER_API_KEY')\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = api_key_value\n",
        "\n",
        "    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "    if not api_key:\n",
        "        print(\"â„¹ï¸  OPENROUTER_API_KEY not found - that's OK for this assignment!\")\n",
        "        print(\"   This assignment only uses local embeddings for vector operations.\")\n",
        "    else:\n",
        "        print(\"âœ… OPENROUTER_API_KEY found - using OpenRouter for LLM operations\")\n",
        "\n",
        "    # Configure local embeddings (no API key required)\n",
        "    Settings.embed_model = HuggingFaceEmbedding(\n",
        "        model_name=\"BAAI/bge-small-en-v1.5\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    print(\"âœ… LlamaIndex configured with local embeddings\")\n",
        "    print(\"   Using BAAI/bge-small-en-v1.5 for document embeddings\")\n",
        "\n",
        "# Setup the configuration\n",
        "setup_llamaindex_settings()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCzEMi030h4Z"
      },
      "source": [
        "## 1. Document Loading Function\n",
        "\n",
        "Complete the function below to load documents from a folder using `SimpleDirectoryReader`.\n",
        "\n",
        "**Note:** This assignment uses local embeddings only - no OpenAI API key required! We're configured to use OpenRouter for future LLM operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-1YzfnJ0h4Z",
        "outputId": "442bf2d2-e2d5-42cd-fd64-d912d498d313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139M/139M [00:02<00:00, 53.4MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TODO: Load documents from /content/drive/MyDrive/ColabNotebooks/data\n",
            "Loaded 42 documents\n"
          ]
        }
      ],
      "source": [
        "def load_documents_from_folder(folder_path: str):\n",
        "    \"\"\"\n",
        "    Load documents from a folder using SimpleDirectoryReader.\n",
        "\n",
        "    TODO: Complete this function to load documents from the given folder path.\n",
        "    HINT: Use SimpleDirectoryReader with recursive parameter to load all files\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder containing documents\n",
        "\n",
        "    Returns:\n",
        "        List of documents loaded from the folder\n",
        "    \"\"\"\n",
        "    # TODO: Create SimpleDirectoryReader instance\n",
        "    # reader = ?\n",
        "\n",
        "    # TODO: Load and return documents\n",
        "    # documents = ?\n",
        "\n",
        "    # return documents\n",
        "    reader = SimpleDirectoryReader(\n",
        "        input_dir=folder_path,\n",
        "        recursive=True\n",
        "    )\n",
        "\n",
        "    documents = reader.load_data()\n",
        "\n",
        "\n",
        "    # PLACEHOLDER - Replace with actual implementation\n",
        "    print(f\"TODO: Load documents from {folder_path}\")\n",
        "    #return []\n",
        "\n",
        "    return documents\n",
        "\n",
        "# Test the function after you complete it\n",
        "# test_folder = \"../data\"\n",
        "test_folder = \"/content/drive/MyDrive/ColabNotebooks/data\"\n",
        "documents = load_documents_from_folder(test_folder)\n",
        "print(f\"Loaded {len(documents)} documents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UvpqdpP86OZl",
        "outputId": "c66884bf-8d2e-4155-fff6-17a731102182",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9POHGhxF0h4a"
      },
      "source": [
        "## 2. Vector Store Creation Function\n",
        "\n",
        "Complete the function below to create a LanceDB vector store.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP8Bx0Qn0h4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbdfc7e0-777b-4151-9c5f-d83307c5447b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.vector_stores.lancedb.base:Table documents doesn't exist yet. Please add some data to create it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TODO: Create vector store at ./assignment_vectordb\n",
            "Vector store created: True\n"
          ]
        }
      ],
      "source": [
        "def create_vector_store(db_path: str = \"./vectordb\", table_name: str = \"documents\"):\n",
        "    \"\"\"\n",
        "    Create a LanceDB vector store for storing document embeddings.\n",
        "\n",
        "    TODO: Complete this function to create and configure a LanceDB vector store.\n",
        "    HINT: Use LanceDBVectorStore with uri and table_name parameters\n",
        "\n",
        "    Args:\n",
        "        db_path (str): Path where the vector database will be stored\n",
        "        table_name (str): Name of the table in the vector database\n",
        "\n",
        "    Returns:\n",
        "        LanceDBVectorStore: Configured vector store\n",
        "    \"\"\"\n",
        "    # TODO: Create the directory if it doesn't exist\n",
        "    # Path(db_path).mkdir(parents=True, exist_ok=True)\n",
        "    Path(db_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # TODO: Create vector store\n",
        "    # vector_store = ?\n",
        "\n",
        "    # return vector_store\n",
        "        # Create directory if it doesn't exist\n",
        "\n",
        "\n",
        "    vector_store = LanceDBVectorStore(\n",
        "        uri=db_path,\n",
        "        table_name=table_name\n",
        "    )\n",
        "    # PLACEHOLDER - Replace with actual implementation\n",
        "    print(f\"TODO: Create vector store at {db_path}\")\n",
        "    return vector_store\n",
        "\n",
        "# Test the function after you complete it\n",
        "vector_store = create_vector_store(\"./assignment_vectordb\")\n",
        "print(f\"Vector store created: {vector_store is not None}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSEzz8Rs0h4a"
      },
      "source": [
        "## 3. Vector Index Creation Function\n",
        "\n",
        "Complete the function below to create a vector index from documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-vKo_BP0h4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c96a8c-7bf1-45b0-eddb-a483fe841bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TODO: Create vector index from 42 documents\n",
            "Vector index created: True\n"
          ]
        }
      ],
      "source": [
        "def create_vector_index(documents: List, vector_store):\n",
        "    \"\"\"\n",
        "    Create a vector index from documents using the provided vector store.\n",
        "\n",
        "    TODO: Complete this function to create a VectorStoreIndex from documents.\n",
        "    HINT: Create StorageContext with vector_store, then use VectorStoreIndex.from_documents()\n",
        "\n",
        "    Args:\n",
        "        documents: List of documents to index\n",
        "        vector_store: LanceDB vector store to use for storage\n",
        "\n",
        "    Returns:\n",
        "        VectorStoreIndex: The created vector index\n",
        "    \"\"\"\n",
        "    # TODO: Create storage context with vector store\n",
        "    # storage_context = ?\n",
        "    storage_context = StorageContext.from_defaults(\n",
        "        vector_store=vector_store\n",
        "    )\n",
        "\n",
        "    # TODO: Create index from documents\n",
        "    # index = ?\n",
        "    index = VectorStoreIndex.from_documents(\n",
        "        documents,\n",
        "        storage_context=storage_context\n",
        "    )\n",
        "    #return index\n",
        "\n",
        "    # PLACEHOLDER - Replace with actual implementation\n",
        "    print(f\"TODO: Create vector index from {len(documents)} documents\")\n",
        "    return index\n",
        "\n",
        "# Test the function after you complete it (will only work after previous functions are completed)\n",
        "if documents and vector_store:\n",
        "    index = create_vector_index(documents, vector_store)\n",
        "    print(f\"Vector index created: {index is not None}\")\n",
        "else:\n",
        "    print(\"Complete previous functions first to test this one\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jbTRMXO0h4a"
      },
      "source": [
        "## 4. Document Search Function\n",
        "\n",
        "Complete the function below to search for relevant documents using the vector index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE_1aIrh0h4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e860d6c-0e17-4208-d157-91ca080f39ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TODO: Search for 'What are AI agents?' in index\n",
            "Found 2 results for query: 'What are AI agents?'\n",
            "Result 1: task. In single agent patterns there is no feedback mechanism from other AI agents; however, there m...\n",
            "Result 2: H. Summary\n",
            "The trend in cloud Python libraries is toward modularity,\n",
            "composability, and seamless int...\n"
          ]
        }
      ],
      "source": [
        "def search_documents(index, query: str, top_k: int = 3):\n",
        "    \"\"\"\n",
        "    Search for relevant documents using the vector index.\n",
        "\n",
        "    TODO: Complete this function to perform semantic search on the index.\n",
        "    HINT: Use index.as_retriever() with similarity_top_k parameter, then retrieve(query)\n",
        "\n",
        "    Args:\n",
        "        index: Vector index to search\n",
        "        query (str): Search query\n",
        "        top_k (int): Number of top results to return\n",
        "\n",
        "    Returns:\n",
        "        List of retrieved document nodes\n",
        "    \"\"\"\n",
        "    # TODO: Create retriever from index\n",
        "    # retriever = ?\n",
        "    retriever = index.as_retriever(\n",
        "        similarity_top_k=top_k\n",
        "    )\n",
        "\n",
        "    # TODO: Retrieve documents for the query\n",
        "    # results = ?\n",
        "    results = retriever.retrieve(query)\n",
        "    # return results\n",
        "\n",
        "    # PLACEHOLDER - Replace with actual implementation\n",
        "    print(f\"TODO: Search for '{query}' in index\")\n",
        "    #return []\n",
        "    return results\n",
        "\n",
        "# Test the function after you complete it (will only work after all previous functions are completed)\n",
        "if 'index' in locals() and index is not None:\n",
        "    test_query = \"What are AI agents?\"\n",
        "    results = search_documents(index, test_query, top_k=2)\n",
        "    print(f\"Found {len(results)} results for query: '{test_query}'\")\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"Result {i}: {result.text[:100] if hasattr(result, 'text') else 'No text'}...\")\n",
        "else:\n",
        "    print(\"Complete all previous functions first to test this one\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYJZ7oKj0h4b"
      },
      "source": [
        "## 5. Final Test - Complete Pipeline\n",
        "\n",
        "Once you've completed all the functions above, run this cell to test the complete pipeline with multiple search queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vToyuFCq0h4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac1e9ba-ab0a-4af1-d411-852e2dee919e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Testing Complete Vector Database Pipeline\n",
            "==================================================\n",
            "\n",
            "ðŸ“‚ Step 1: Loading documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TODO: Load documents from /content/drive/MyDrive/ColabNotebooks/data\n",
            "   Loaded 42 documents\n",
            "\n",
            "ðŸ—„ï¸ Step 2: Creating vector store...\n",
            "TODO: Create vector store at ./assignment_vectordb\n",
            "   Vector store status: âœ… Created\n",
            "\n",
            "ðŸ”— Step 3: Creating vector index...\n",
            "TODO: Create vector index from 42 documents\n",
            "   Index status: âœ… Created\n",
            "\n",
            "ðŸ” Step 4: Testing search functionality...\n",
            "\n",
            "   ðŸ”Ž Query: 'What are AI agents?'\n",
            "TODO: Search for 'What are AI agents?' in index\n",
            "      1. task. In single agent patterns there is no feedback mechanism from other AI agents; however, there m... (Score: 0.6169)\n",
            "      2. task. In single agent patterns there is no feedback mechanism from other AI agents; however, there m... (Score: 0.6169)\n",
            "\n",
            "   ðŸ”Ž Query: 'How to evaluate agent performance?'\n",
            "TODO: Search for 'How to evaluate agent performance?' in index\n",
            "      1. steps, but the answers are limited to Yes/No responses [7]. As the industry continues to pivot towar... (Score: 0.6866)\n",
            "      2. steps, but the answers are limited to Yes/No responses [7]. As the industry continues to pivot towar... (Score: 0.6866)\n",
            "\n",
            "   ðŸ”Ž Query: 'Italian recipes and cooking'\n",
            "TODO: Search for 'Italian recipes and cooking' in index\n",
            "      1. Spaghetti Carbonara, Italian, 20, Easy, Pasta, 450\n",
            "Margherita Pizza, Italian, 45, Medium, Tomato, 32... (Score: 0.6234)\n",
            "      2. Spaghetti Carbonara, Italian, 20, Easy, Pasta, 450\n",
            "Margherita Pizza, Italian, 45, Medium, Tomato, 32... (Score: 0.6234)\n",
            "\n",
            "   ðŸ”Ž Query: 'Financial analysis and investment'\n",
            "TODO: Search for 'Financial analysis and investment' in index\n",
            "      1. Stock, AAPL, Apple Inc, 10000, 12500, 25.0, Medium\n",
            "Stock, GOOGL, Alphabet Inc, 8000, 9200, 15.0, Med... (Score: 0.5756)\n",
            "      2. Stock, AAPL, Apple Inc, 10000, 12500, 25.0, Medium\n",
            "Stock, GOOGL, Alphabet Inc, 8000, 9200, 15.0, Med... (Score: 0.5756)\n",
            "\n",
            "==================================================\n",
            "ðŸŽ¯ Assignment Status:\n",
            "   Documents loaded: âœ…\n",
            "   Vector store created: âœ…\n",
            "   Index created: âœ…\n",
            "   Search working: âœ…\n",
            "\n",
            "ðŸŽ‰ Congratulations! You've successfully completed the assignment!\n",
            "   You've built a complete vector database with search functionality!\n"
          ]
        }
      ],
      "source": [
        "# Final test of the complete pipeline\n",
        "print(\"ðŸš€ Testing Complete Vector Database Pipeline\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Re-run the complete pipeline to ensure everything works\n",
        "data_folder = \"/content/drive/MyDrive/ColabNotebooks/data\"\n",
        "vector_db_path = \"./assignment_vectordb\"\n",
        "\n",
        "# Step 1: Load documents\n",
        "print(\"\\nðŸ“‚ Step 1: Loading documents...\")\n",
        "documents = load_documents_from_folder(data_folder)\n",
        "print(f\"   Loaded {len(documents)} documents\")\n",
        "\n",
        "# Step 2: Create vector store\n",
        "print(\"\\nðŸ—„ï¸ Step 2: Creating vector store...\")\n",
        "vector_store = create_vector_store(vector_db_path)\n",
        "print(\"   Vector store status:\", \"âœ… Created\" if vector_store else \"âŒ Failed\")\n",
        "\n",
        "# Step 3: Create vector index\n",
        "print(\"\\nðŸ”— Step 3: Creating vector index...\")\n",
        "if documents and vector_store:\n",
        "    index = create_vector_index(documents, vector_store)\n",
        "    print(\"   Index status:\", \"âœ… Created\" if index else \"âŒ Failed\")\n",
        "else:\n",
        "    index = None\n",
        "    print(\"   âŒ Cannot create index - missing documents or vector store\")\n",
        "\n",
        "# Step 4: Test multiple search queries\n",
        "print(\"\\nðŸ” Step 4: Testing search functionality...\")\n",
        "if index:\n",
        "    search_queries = [\n",
        "        \"What are AI agents?\",\n",
        "        \"How to evaluate agent performance?\",\n",
        "        \"Italian recipes and cooking\",\n",
        "        \"Financial analysis and investment\"\n",
        "    ]\n",
        "\n",
        "    for query in search_queries:\n",
        "        print(f\"\\n   ðŸ”Ž Query: '{query}'\")\n",
        "        results = search_documents(index, query, top_k=2)\n",
        "\n",
        "        if results:\n",
        "            for i, result in enumerate(results, 1):\n",
        "                text_preview = result.text[:100] if hasattr(result, 'text') else \"No text available\"\n",
        "                score = f\" (Score: {result.score:.4f})\" if hasattr(result, 'score') else \"\"\n",
        "                print(f\"      {i}. {text_preview}...{score}\")\n",
        "        else:\n",
        "            print(\"      No results found\")\n",
        "else:\n",
        "    print(\"   âŒ Cannot test search - index not created\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ðŸŽ¯ Assignment Status:\")\n",
        "print(f\"   Documents loaded: {'âœ…' if documents else 'âŒ'}\")\n",
        "print(f\"   Vector store created: {'âœ…' if vector_store else 'âŒ'}\")\n",
        "print(f\"   Index created: {'âœ…' if index else 'âŒ'}\")\n",
        "print(f\"   Search working: {'âœ…' if index else 'âŒ'}\")\n",
        "\n",
        "if documents and vector_store and index:\n",
        "    print(\"\\nðŸŽ‰ Congratulations! You've successfully completed the assignment!\")\n",
        "    print(\"   You've built a complete vector database with search functionality!\")\n",
        "else:\n",
        "    print(\"\\nðŸ“ Please complete the TODO functions above to finish the assignment.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5b8b2b76a13848668080f0bcf791e5f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d351575db0f14dbeb63cacd1a7baac4b",
              "IPY_MODEL_879409bd67fa4896bbec7570735bdb0a",
              "IPY_MODEL_719eff8d8b0441dfadf1dd936c793637"
            ],
            "layout": "IPY_MODEL_83bea923e4f7404d9eedfe486b688a3e"
          }
        },
        "d351575db0f14dbeb63cacd1a7baac4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07e347f328504c91b91d2edf5cc11c5f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_160e206f2ade458d942ded44263ec712",
            "value": "Loadingâ€‡weights:â€‡100%"
          }
        },
        "879409bd67fa4896bbec7570735bdb0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae915866cb8545488890c6629d1012b2",
            "max": 199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5a5e097e8c94dcabf6b114e90456a1b",
            "value": 199
          }
        },
        "719eff8d8b0441dfadf1dd936c793637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70610deda338473598c905f6648ab8d8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b34bf5cdcc344a10b88df63cebcf6973",
            "value": "â€‡199/199â€‡[00:00&lt;00:00,â€‡486.37it/s,â€‡Materializingâ€‡param=pooler.dense.weight]"
          }
        },
        "83bea923e4f7404d9eedfe486b688a3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07e347f328504c91b91d2edf5cc11c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "160e206f2ade458d942ded44263ec712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae915866cb8545488890c6629d1012b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5a5e097e8c94dcabf6b114e90456a1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70610deda338473598c905f6648ab8d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b34bf5cdcc344a10b88df63cebcf6973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}