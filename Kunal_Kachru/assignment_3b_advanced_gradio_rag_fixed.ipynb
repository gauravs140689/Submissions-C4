{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u6NBAJtcWOR"
      },
      "source": [
        "# Assignment 3b: Advanced Gradio RAG Frontend\n",
        "## Day 6 Session 2 - Building Configurable RAG Applications\n",
        "\n",
        "In this assignment, you'll extend your basic RAG interface with advanced configuration options to create a professional, feature-rich RAG application.\n",
        "\n",
        "**New Features to Add:**\n",
        "- Model selection dropdown (gpt-4o, gpt-4o-mini)\n",
        "- Temperature slider (0 to 1 with 0.1 intervals)\n",
        "- Chunk size configuration\n",
        "- Chunk overlap configuration  \n",
        "- Similarity top-k slider\n",
        "- Node postprocessor multiselect\n",
        "- Similarity cutoff slider\n",
        "- Response synthesizer multiselect\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Advanced Gradio components and interactions\n",
        "- Dynamic RAG configuration\n",
        "- Professional UI design patterns\n",
        "- Parameter validation and handling\n",
        "- Building production-ready AI applications\n",
        "\n",
        "**Prerequisites:**\n",
        "- Completed Assignment 3a (Basic Gradio RAG)\n",
        "- Understanding of RAG parameters and their effects\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9vbXaljcpua",
        "outputId": "a028eef3-11af-4cc3-9d01-50eda91d9229"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r \"/content/drive/MyDrive/ColabNotebooks/requirements.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shXCKMGGczcQ",
        "outputId": "09326e2f-d201-47c5-db4d-e757f26b8364"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m358.4/803.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: huggingface-hub 1.3.7 does not provide the extra 'inference'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.8/96.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "api_key_value = userdata.get('OPENROUTER_API_KEY')\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = api_key_value\n",
        "\n",
        "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "if not api_key:\n",
        "    print(\"â„¹ï¸  OPENROUTER_API_KEY not found - that's OK for this assignment!\")\n",
        "    print(\"   This assignment only uses local embeddings for vector operations.\")\n",
        "else:\n",
        "    print(\"âœ… OPENROUTER_API_KEY found - using OpenRouter for LLM operations\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9Y6WYdcdGUh",
        "outputId": "b26f5115-b173-4c0f-d9b0-abf32d00d256"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… OPENROUTER_API_KEY found - using OpenRouter for LLM operations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3TLLNUBcWOT"
      },
      "source": [
        "## ğŸ“š Part 1: Setup and Imports\n",
        "\n",
        "Import all necessary libraries including advanced RAG components for configuration options.\n",
        "\n",
        "**Note:** This assignment uses OpenRouter for LLM access (not OpenAI). Make sure you have your `OPENROUTER_API_KEY` environment variable set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSN3XPcfcWOT",
        "outputId": "64770740-846b-4e03-f93b-b1f9ad8c76df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import all required libraries\n",
        "import gradio as gr\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "# LlamaIndex core components\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n",
        "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.openrouter import OpenRouter\n",
        "\n",
        "# Advanced RAG components\n",
        "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
        "from llama_index.core.response_synthesizers import TreeSummarize, Refine, CompactAndRefine\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-17lgp3ZcWOU"
      },
      "source": [
        "## ğŸ¤– Part 2: Advanced RAG Backend Class\n",
        "\n",
        "Create an advanced RAG backend that supports dynamic configuration of all parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707,
          "referenced_widgets": [
            "18a8c94ae022461b992d6d82ed9dcf92",
            "c45f9e84cb1e4f71b5a4536fbb4e8fa8",
            "000808d4f77c44d0a0f58bf1d10a89b9",
            "afba869736414c9ead4eb62ae5a008c3",
            "12788a742324486cae16dfd0734468ed",
            "4cafbcb92b3c468899727c73520a00e9",
            "ed3c2270bce447fc93fa3ff6b20008c2",
            "09ebb530c3a043f69dd7552f6c3d707f",
            "0b946a818ccb4d85b01f6da41e89484e",
            "ef49058261654a64aa1e8ec048c156dd",
            "068243c48848414a97ced1fa7194ddb7",
            "7567d497814a4913a440473f0e537c38",
            "b6b1ce93683448cf92d658a65649ce58",
            "f78fae5275f04b1e9f3acfbd9cc43cf9",
            "f2bbfdb0e996482183b923c0a68a43a8",
            "8e4b50963cde4687a9c6eb35a789668c",
            "4af49e2a7c024dfb83c0673d4bc94472",
            "7641efae4827410c9d5055801ede5959",
            "84bd95d6b9d2426a9c11302881f385d6",
            "113f0f35ef5248fda3b60599bbe60410",
            "bc884c797b1c4430a49090853a6ad10e",
            "40c332a7b042417ead80ecae7dd08f19",
            "7cd56ac2061c47b28d4e7ca659d8d958",
            "a64fcbc6431d41429bc3ee69ae754dde",
            "477459d296a44a58ab040b9f32acaf90",
            "bd1bf391b5544f9fbe92bdefa08e8ef9",
            "f38378f07daa45f5a67cc46a3b385e8d",
            "0764bbe84f3c459d90c9740b7c313f2b",
            "5fd96d20378a4aec86b338c90f89ca66",
            "caa22107e76b4c658c55b27e1cc5c747",
            "26f0b60b5c7544d4a6b9244a96666021",
            "c4cfd53b36114a608250e1e64d0e1e00",
            "a09ecf86883445aebb4a101bd7c928b3",
            "c0fc326255ea44019359509035494c15",
            "7f7d3188a04144529530d36d604610c3",
            "b9ad92cf9aab4d9cb292d6f81eae2ce5",
            "7da27dde741b45e9ab43d66c6ff230c0",
            "34e2094a616147b7a1eec318eeef4542",
            "ba79761914094787a04a43b992fe79c6",
            "e96748d6e7744ca9a10fdaeb84fb9371",
            "70af451e21c348db836cbdd50364b53f",
            "6e486b89a5414d7997a4e5d9da2bfb05",
            "d96eebba30be43dfa8b1b5f8db3489df",
            "197fded23a994e73a2cbe7477004400e",
            "85320580bf17436994e3933a4930a03b",
            "e2a61c13beab444da17d2022d2b9f899",
            "6b0367d3b8b44aa1ad84444d55e15623",
            "730ddfbfb0234795bda06f4d88cf1520",
            "7e5aec37bce44a5aba4aa30309b69fbe",
            "5d1760af11bd429daa715d1096f734ed",
            "0907e6556bd2443094178b4f5b330b83",
            "f0bab8ae98384dc1973dace72811ac8d",
            "0a339ebbe66e47f6ba992f93058c98bb",
            "30a0a8756bec4385b872aefa3f24c97e",
            "bcdb26d600264eff90de4b7f48f5c84e",
            "21177f41f74b45d1bbec30dbb951e204",
            "c00f4d8cd22644eeaccb64eecee3fb2a",
            "f004a67cef5d4f0ba5de78af9c0835e9",
            "364fbf03455b4ccb8de1592eb6e5e9ff",
            "82171297a2104b3ba1c7a7967fa03f3e",
            "b3c17aa8aa6f4a7b8617d5d4351ed770",
            "7dcd79b099e64c66aa797a03dead2139",
            "14f51fc1275a4a4ab0f9e36120afab70",
            "0d21925d68f6490298e000a02bc9b366",
            "64550e6b35f84786b972aff5446bc97a",
            "bb177ffcee4448389703ace34c208106",
            "e25efc114c1b480885a15d07a1714fee",
            "0594504bcaa04510ae627900ff0f05cc",
            "7fcf0aae0f384365b5c3d84bc3e8195a",
            "2a2e3d0d16ce4e87b3f81cf809c67f15",
            "1847556f4e0f4d5f95b089bf6461a214",
            "7af46369186f4808a394da682e4b3764",
            "1f2d0873e2ce47ee86914df4051498a8",
            "c966f210636042678fa4b75893800f9f",
            "5c01c2bf8a974e298e7906adf251266a",
            "cae73da5320e465987761e339088277b",
            "465b6df8c89a4a9196ae3ff0801a8fe5",
            "4f9962e4419149838b54904818e819ac",
            "8e44fb6776a94de08d4f9cf2adedb239",
            "42c15b06b0d24c2a84fe2e44e8d43888",
            "5ed0d442d026499a851133240e545c0d",
            "e2bd6ce16a5049f6975f2ad6725bf881",
            "5ba08267989c4b5f9a813acebd84f588",
            "05fff56c7eb945ccb3ebf640028da9da",
            "aab7514052d54063a95820609a8dc4b2",
            "de8c9665a3384a6aa1f16ffedfb9c553",
            "44ed0761d44a4d61be86f1ce654754ee",
            "007b9a979aca4db7b644aec17f497887",
            "d754edce39e14ad5833d0463986f7336",
            "e4ee4826aa834755a9de92fde2930394",
            "4e8bec7c59804865adebc9310d36483b",
            "a52c7c7f8d344e0db0ae12867f9c6489",
            "df2a113bcc5940cb87a85da57ea863dd",
            "32c432e06ce84bd2bb9f212580e323a6",
            "a4e38386d4b14a7087784ea76e422d09",
            "4b6d6f2b9bec46e695fcbccf21aed0f7",
            "30e9fe94667044e786fc12019759310f",
            "edacc0bb59644f59b2190f216b98252f",
            "0ded1e0190a1409ea9e16aea632b0bff",
            "6f3ac26294df4ca9b18f1fc7dd0dcc46",
            "d579c95615a94320b7316a32c6a59b34",
            "179787dbab2f43e395a2e3243c526603",
            "bc16f7efbf9b4ff5918b598c283cd61b",
            "8c4fbcf9e5e04c77821081a12fae82f6",
            "9bb72af340db46fb86ff7cf86c619fdf",
            "f1b75124881d4470b2b7ca33e225ac54",
            "e304baa608e0423f9c9853b2bf9afa1a",
            "ece2c13c09cb4f3682716b97a06c8a21",
            "6048f365773f48d1ae3bbf54b987a666",
            "9cc0973a15ef487c8b3d9ecc7dd3fd26",
            "6493791e264849919caf82d3272954a5",
            "5418f85c1db24d9ba244e246d86f9df8",
            "698358f282ad408a91b1e611c2ce0c66",
            "117e5aef299f4cdba0b3cdefa5dd26cd",
            "8a90fb95a1c342889293379aaf39568e",
            "c2c5c6acc8b14b3f85e915e485a01874",
            "afb6590bcec948fbacff233f3801f8ff",
            "d728822b0a9f4db5a36e24fc4500080b",
            "d22baac73519484fb143d4e51e211038",
            "880dcacabef54d8795f2fce54d7bd608",
            "f6354e20da9445a48ed4d6ac064e18d9",
            "3d0f2c12ceac4f948d00ae14a0246e81",
            "e8d92374e87e473e8b8e7bc2c5d4b774",
            "1fdabdd066034992b4081cccd3292e08",
            "ab651e03770046a2aa65f46f066db5d3",
            "a6fe13b72b794a2aa4d1ea21fd70b7d8",
            "3a9d4a5f6f204af6b3b92c28de9703aa",
            "2c29a93c6ac145c881ea4da0fcc2451b",
            "941a50ce4fb84c16bd2d350cb27cf918",
            "493a80c0006d4a51a388773c09ffac95",
            "7fdb5bb385f5457cb3dccf92037bb1e9",
            "377b883b58b44e62b13ef76f5bde9956"
          ]
        },
        "id": "aaXbwBoQcWOV",
        "outputId": "65cf8a34-7821-4e69-abef-8dd7ef3dfc71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "18a8c94ae022461b992d6d82ed9dcf92"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7567d497814a4913a440473f0e537c38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cd56ac2061c47b28d4e7ca659d8d958"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0fc326255ea44019359509035494c15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85320580bf17436994e3933a4930a03b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21177f41f74b45d1bbec30dbb951e204"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e25efc114c1b480885a15d07a1714fee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: BAAI/bge-small-en-v1.5\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f9962e4419149838b54904818e819ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d754edce39e14ad5833d0463986f7336"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f3ac26294df4ca9b18f1fc7dd0dcc46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6493791e264849919caf82d3272954a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d0f2c12ceac4f948d00ae14a0246e81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Advanced RAG Backend initialized and ready!\n"
          ]
        }
      ],
      "source": [
        "class AdvancedRAGBackend:\n",
        "    \"\"\"Advanced RAG backend with configurable parameters.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.index = None\n",
        "        self.available_models = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
        "        self.available_postprocessors = [\"SimilarityPostprocessor\"]\n",
        "        self.available_synthesizers = [\"TreeSummarize\", \"Refine\", \"CompactAndRefine\", \"Default\"]\n",
        "        self.update_settings()\n",
        "\n",
        "    def update_settings(self, model: str = \"gpt-4o-mini\", temperature: float = 0.1, chunk_size: int = 512, chunk_overlap: int = 50):\n",
        "        \"\"\"Update LlamaIndex settings based on user configuration.\"\"\"\n",
        "        # Set up the LLM using OpenRouter\n",
        "        api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "        if api_key:\n",
        "            Settings.llm = OpenRouter(\n",
        "                api_key=api_key,\n",
        "                model=model,\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "        # Set up the embedding model (keep this constant)\n",
        "        Settings.embed_model = HuggingFaceEmbedding(\n",
        "            model_name=\"BAAI/bge-small-en-v1.5\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Set chunking parameters from function parameters\n",
        "        Settings.chunk_size = chunk_size\n",
        "        Settings.chunk_overlap = chunk_overlap\n",
        "\n",
        "    def initialize_database(self, data_folder=\"/content/drive/MyDrive/ColabNotebooks/data\"):\n",
        "        \"\"\"Initialize the vector database with documents.\"\"\"\n",
        "        # Check if data folder exists\n",
        "        if not Path(data_folder).exists():\n",
        "            return f\"âŒ Data folder '{data_folder}' not found!\"\n",
        "\n",
        "        try:\n",
        "            # Create vector store\n",
        "            vector_store = LanceDBVectorStore(\n",
        "                uri=\"./advanced_rag_vectordb\",\n",
        "                table_name=\"documents\"\n",
        "            )\n",
        "\n",
        "            # Load documents\n",
        "            reader = SimpleDirectoryReader(input_dir=data_folder, recursive=True)\n",
        "            documents = reader.load_data()\n",
        "\n",
        "            # Create storage context and index\n",
        "            storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "            self.index = VectorStoreIndex.from_documents(\n",
        "                documents,\n",
        "                storage_context=storage_context,\n",
        "                show_progress=True\n",
        "            )\n",
        "\n",
        "            return f\"âœ… Database initialized successfully with {len(documents)} documents!\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"âŒ Error initializing database: {str(e)}\"\n",
        "\n",
        "    def get_postprocessor(self, postprocessor_name: str, similarity_cutoff: float):\n",
        "        \"\"\"Get the selected postprocessor.\"\"\"\n",
        "        if postprocessor_name == \"SimilarityPostprocessor\":\n",
        "            return SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)\n",
        "        elif postprocessor_name == \"None\":\n",
        "            return None\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def get_synthesizer(self, synthesizer_name: str):\n",
        "        \"\"\"Get the selected response synthesizer.\"\"\"\n",
        "        if synthesizer_name == \"TreeSummarize\":\n",
        "            return TreeSummarize()\n",
        "        elif synthesizer_name == \"Refine\":\n",
        "            return Refine()\n",
        "        elif synthesizer_name == \"CompactAndRefine\":\n",
        "            return CompactAndRefine()\n",
        "        elif synthesizer_name == \"Default\":\n",
        "            return None\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def advanced_query(self, question: str, model: str, temperature: float,\n",
        "                      chunk_size: int, chunk_overlap: int, similarity_top_k: int,\n",
        "                      postprocessor_names: List[str], similarity_cutoff: float,\n",
        "                      synthesizer_name: str) -> Dict[str, Any]:\n",
        "        \"\"\"Query the RAG system with advanced configuration.\"\"\"\n",
        "\n",
        "        # Check if index exists\n",
        "        if self.index is None:\n",
        "            return {\"response\": \"âŒ Please initialize the database first!\", \"sources\": [], \"config\": {}}\n",
        "\n",
        "        # Check if question is empty\n",
        "        if not question or not question.strip():\n",
        "            return {\"response\": \"âš ï¸ Please enter a question first!\", \"sources\": [], \"config\": {}}\n",
        "\n",
        "        try:\n",
        "            # Update settings with new parameters\n",
        "            self.update_settings(model, temperature, chunk_size, chunk_overlap)\n",
        "\n",
        "            # Get postprocessors\n",
        "            postprocessors = []\n",
        "            for name in postprocessor_names:\n",
        "                processor = self.get_postprocessor(name, similarity_cutoff)\n",
        "                if processor is not None:\n",
        "                    postprocessors.append(processor)\n",
        "\n",
        "            # Get synthesizer\n",
        "            synthesizer = self.get_synthesizer(synthesizer_name)\n",
        "\n",
        "            # Create query engine with all parameters\n",
        "            query_engine_kwargs = {\"similarity_top_k\": similarity_top_k}\n",
        "            if postprocessors:\n",
        "                query_engine_kwargs[\"node_postprocessors\"] = postprocessors\n",
        "            if synthesizer is not None:\n",
        "                query_engine_kwargs[\"response_synthesizer\"] = synthesizer\n",
        "\n",
        "            query_engine = self.index.as_query_engine(**query_engine_kwargs)\n",
        "\n",
        "            # Query and get response\n",
        "            response = query_engine.query(question)\n",
        "\n",
        "            # Extract source information if available\n",
        "            sources = []\n",
        "            if hasattr(response, 'source_nodes'):\n",
        "                for node in response.source_nodes:\n",
        "                    sources.append({\n",
        "                        \"text\": node.text[:200] + \"...\",\n",
        "                        \"score\": getattr(node, 'score', 0.0),\n",
        "                        \"source\": getattr(node.node, 'metadata', {}).get('file_name', 'Unknown')\n",
        "                    })\n",
        "\n",
        "            return {\n",
        "                \"response\": str(response),\n",
        "                \"sources\": sources,\n",
        "                \"config\": {\n",
        "                    \"model\": model,\n",
        "                    \"temperature\": temperature,\n",
        "                    \"chunk_size\": chunk_size,\n",
        "                    \"chunk_overlap\": chunk_overlap,\n",
        "                    \"similarity_top_k\": similarity_top_k,\n",
        "                    \"postprocessors\": postprocessor_names,\n",
        "                    \"similarity_cutoff\": similarity_cutoff,\n",
        "                    \"synthesizer\": synthesizer_name\n",
        "                }\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"response\": f\"âŒ Error processing query: {str(e)}\", \"sources\": [], \"config\": {}}\n",
        "\n",
        "# Initialize the backend\n",
        "rag_backend = AdvancedRAGBackend()\n",
        "print(\"ğŸš€ Advanced RAG Backend initialized and ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxqly_FucWOV"
      },
      "source": [
        "## ğŸ¨ Part 3: Advanced Gradio Interface\n",
        "\n",
        "Create a sophisticated Gradio interface with all the configuration options specified:\n",
        "1. Database initialization button\n",
        "2. Search query input and button  \n",
        "3. Model selection dropdown\n",
        "4. Temperature slider\n",
        "5. Chunk size and overlap inputs\n",
        "6. Similarity top-k slider\n",
        "7. Node postprocessor multiselect\n",
        "8. Similarity cutoff slider\n",
        "9. Response synthesizer multiselect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCi3H4Z9cWOV",
        "outputId": "81328e09-7acb-4fe3-ad22-d7d96aca1927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Advanced RAG interface created successfully!\n"
          ]
        }
      ],
      "source": [
        "def create_advanced_rag_interface():\n",
        "    \"\"\"Create advanced RAG interface with full configuration options.\"\"\"\n",
        "\n",
        "    def initialize_db():\n",
        "        \"\"\"Handle database initialization.\"\"\"\n",
        "        return rag_backend.initialize_database()\n",
        "\n",
        "    def handle_advanced_query(question, model, temperature, chunk_size, chunk_overlap,\n",
        "                             similarity_top_k, postprocessors, similarity_cutoff, synthesizer):\n",
        "        \"\"\"Handle advanced RAG queries with all configuration options.\"\"\"\n",
        "        result = rag_backend.advanced_query(\n",
        "            question, model, temperature, chunk_size, chunk_overlap,\n",
        "            similarity_top_k, postprocessors, similarity_cutoff, synthesizer\n",
        "        )\n",
        "\n",
        "        # Format configuration for display\n",
        "        config_text = f\"\"\"**Current Configuration:**\n",
        "- Model: {result['config'].get('model', 'N/A')}\n",
        "- Temperature: {result['config'].get('temperature', 'N/A')}\n",
        "- Chunk Size: {result['config'].get('chunk_size', 'N/A')}\n",
        "- Chunk Overlap: {result['config'].get('chunk_overlap', 'N/A')}\n",
        "- Similarity Top-K: {result['config'].get('similarity_top_k', 'N/A')}\n",
        "- Postprocessors: {', '.join(result['config'].get('postprocessors', []))}\n",
        "- Similarity Cutoff: {result['config'].get('similarity_cutoff', 'N/A')}\n",
        "- Synthesizer: {result['config'].get('synthesizer', 'N/A')}\"\"\"\n",
        "\n",
        "        return result[\"response\"], config_text, gr.update(value=result.get(\"sources\", []))\n",
        "\n",
        "    with gr.Blocks(title=\"Advanced RAG Assistant\") as interface:\n",
        "        gr.Markdown(\"# ğŸš€ Advanced RAG Assistant with Dynamic Configuration\")\n",
        "        gr.Markdown(\"Explore the power of Retrieval-Augmented Generation with configurable parameters.\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                init_btn = gr.Button(\"Initialize Vector Database\", variant=\"primary\")\n",
        "                status_output = gr.Textbox(label=\"Database Status\", interactive=False)\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "\n",
        "                gr.Markdown(\"### âš™ï¸ RAG Configuration\")\n",
        "\n",
        "                model_dropdown = gr.Dropdown(\n",
        "                    label=\"Model Selection\",\n",
        "                    choices=rag_backend.available_models,\n",
        "                    value=\"gpt-4o-mini\",\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "                temperature_slider = gr.Slider(\n",
        "                    label=\"Temperature\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=1.0,\n",
        "                    step=0.1,\n",
        "                    value=0.1,\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "                chunk_size_input = gr.Number(\n",
        "                    label=\"Chunk Size\",\n",
        "                    value=512,\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "                chunk_overlap_input = gr.Number(\n",
        "                    label=\"Chunk Overlap\",\n",
        "                    value=50,\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "                similarity_topk_slider = gr.Slider(\n",
        "                    label=\"Similarity Top-K\",\n",
        "                    minimum=1,\n",
        "                    maximum=20,\n",
        "                    step=1,\n",
        "                    value=5,\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "                postprocessor_checkbox = gr.CheckboxGroup(\n",
        "                    label=\"Node Postprocessors\",\n",
        "                    choices=rag_backend.available_postprocessors,\n",
        "                    value=[], # Default to no postprocessors selected\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "                similarity_cutoff_slider = gr.Slider(\n",
        "                    label=\"Similarity Cutoff (for Postprocessor)\",\n",
        "                    minimum=0.0,\n",
        "                    maximum=1.0,\n",
        "                    step=0.05,\n",
        "                    value=0.3,\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "                synthesizer_dropdown = gr.Dropdown(\n",
        "                    label=\"Response Synthesizer\",\n",
        "                    choices=rag_backend.available_synthesizers,\n",
        "                    value=\"Default\",\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### ğŸ’¬ Query Interface\")\n",
        "\n",
        "                query_input = gr.Textbox(\n",
        "                    label=\"Ask a question\",\n",
        "                    placeholder=\"E.g., What are the main features of the product?\",\n",
        "                    lines=3,\n",
        "                    interactive=True\n",
        "                )\n",
        "\n",
        "                submit_btn = gr.Button(\"Ask Question\", variant=\"primary\")\n",
        "\n",
        "                response_output = gr.Textbox(\n",
        "                    label=\"Response\",\n",
        "                    lines=12,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "                gr.Markdown(\"### ğŸ“„ Source Nodes\")\n",
        "                # Removed 'interactive=False' as gr.JSON does not support it.\n",
        "                source_nodes_output = gr.JSON(label=\"Source Nodes\", visible=True)\n",
        "\n",
        "                gr.Markdown(\"### ğŸ“Š Configuration Used\")\n",
        "                config_display = gr.Textbox(\n",
        "                    label=\"Current Configuration Details\",\n",
        "                    lines=8,\n",
        "                    interactive=False\n",
        "                )\n",
        "\n",
        "        init_btn.click(initialize_db, outputs=[status_output])\n",
        "\n",
        "        submit_btn.click(\n",
        "            handle_advanced_query,\n",
        "            inputs=[\n",
        "                query_input, model_dropdown, temperature_slider,\n",
        "                chunk_size_input, chunk_overlap_input, similarity_topk_slider,\n",
        "                postprocessor_checkbox, similarity_cutoff_slider, synthesizer_dropdown\n",
        "            ],\n",
        "            outputs=[response_output, config_display, source_nodes_output]\n",
        "        )\n",
        "\n",
        "\n",
        "    return interface\n",
        "\n",
        "# Create the interface\n",
        "advanced_interface = create_advanced_rag_interface()\n",
        "print(\"âœ… Advanced RAG interface created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnJOuzi8cWOW"
      },
      "source": [
        "## ğŸš€ Part 4: Launch Your Advanced Application\n",
        "\n",
        "Launch your advanced Gradio application and test all the configuration options!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qEN-tzZVcWOW",
        "outputId": "6ba6a53a-0b34-4930-dd04-89e433cc388c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ‰ Launching your Advanced RAG Assistant...\n",
            "ğŸ”— Your application will open in a new browser tab!\n",
            "\n",
            "âš ï¸  Make sure your OPENROUTER_API_KEY environment variable is set!\n",
            "\n",
            "ğŸ“‹ Testing Instructions:\n",
            "1. Click 'Initialize Vector Database' button first\n",
            "2. Wait for success message\n",
            "3. Configure your RAG parameters:\n",
            "   - Choose model (gpt-4o, gpt-4o-mini)\n",
            "   - Adjust temperature (0.0 = deterministic, 1.0 = creative)\n",
            "   - Set chunk size and overlap\n",
            "   - Choose similarity top-k\n",
            "   - Select postprocessors and synthesizer\n",
            "4. Enter a question and click 'Ask Question'\n",
            "5. Review both the response and configuration used\n",
            "\n",
            "ğŸ§ª Experiments to try:\n",
            "- Compare different models with the same question\n",
            "- Test temperature effects (0.1 vs 0.9)\n",
            "- Try different chunk sizes (256 vs 1024)\n",
            "- Compare synthesizers (TreeSummarize vs Refine)\n",
            "- Adjust similarity cutoff to filter results\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://456ece358d03352601.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://456ece358d03352601.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "print(\"ğŸ‰ Launching your Advanced RAG Assistant...\")\n",
        "print(\"ğŸ”— Your application will open in a new browser tab!\")\n",
        "print(\"\")\n",
        "print(\"âš ï¸  Make sure your OPENROUTER_API_KEY environment variable is set!\")\n",
        "print(\"\")\n",
        "print(\"ğŸ“‹ Testing Instructions:\")\n",
        "print(\"1. Click 'Initialize Vector Database' button first\")\n",
        "print(\"2. Wait for success message\")\n",
        "print(\"3. Configure your RAG parameters:\")\n",
        "print(\"   - Choose model (gpt-4o, gpt-4o-mini)\")\n",
        "print(\"   - Adjust temperature (0.0 = deterministic, 1.0 = creative)\")\n",
        "print(\"   - Set chunk size and overlap\")\n",
        "print(\"   - Choose similarity top-k\")\n",
        "print(\"   - Select postprocessors and synthesizer\")\n",
        "print(\"4. Enter a question and click 'Ask Question'\")\n",
        "print(\"5. Review both the response and configuration used\")\n",
        "print(\"\")\n",
        "print(\"ğŸ§ª Experiments to try:\")\n",
        "print(\"- Compare different models with the same question\")\n",
        "print(\"- Test temperature effects (0.1 vs 0.9)\")\n",
        "print(\"- Try different chunk sizes (256 vs 1024)\")\n",
        "print(\"- Compare synthesizers (TreeSummarize vs Refine)\")\n",
        "print(\"- Adjust similarity cutoff to filter results\")\n",
        "\n",
        "# Your code here:\n",
        "advanced_interface.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86crwt4IcWOW"
      },
      "source": [
        "## ğŸ’¡ Understanding the Configuration Options\n",
        "\n",
        "### Model Selection\n",
        "- **gpt-4o**: Latest and most capable model, best quality responses\n",
        "- **gpt-4o-mini**: Faster and cheaper while maintaining good quality\n",
        "\n",
        "### Temperature (0.0 - 1.0)\n",
        "- **0.0-0.3**: Deterministic, factual responses\n",
        "- **0.4-0.7**: Balanced creativity and accuracy\n",
        "- **0.8-1.0**: More creative and varied responses\n",
        "\n",
        "### Chunk Size & Overlap\n",
        "- **Chunk Size**: How much text to process at once (256-1024 typical)\n",
        "- **Chunk Overlap**: Overlap between chunks to maintain context (10-100 typical)\n",
        "\n",
        "### Similarity Top-K (1-20)\n",
        "- **Lower values (3-5)**: More focused, faster responses\n",
        "- **Higher values (8-15)**: More comprehensive, detailed responses\n",
        "\n",
        "### Node Postprocessors\n",
        "- **SimilarityPostprocessor**: Filters out low-relevance documents\n",
        "\n",
        "### Similarity Cutoff (0.0-1.0)\n",
        "- **0.1-0.3**: More permissive, includes potentially relevant docs\n",
        "- **0.5-0.8**: More strict, only highly relevant docs\n",
        "\n",
        "### Response Synthesizers\n",
        "- **TreeSummarize**: Hierarchical summarization, good for complex topics\n",
        "- **Refine**: Iterative refinement, builds detailed responses\n",
        "- **CompactAndRefine**: Efficient version of Refine\n",
        "- **Default**: Standard synthesis approach\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUNxgTyTcWOW"
      },
      "source": [
        "## âœ… Assignment Completion Checklist\n",
        "\n",
        "Before submitting, ensure you have:\n",
        "\n",
        "- [ ] Set up your OPENROUTER_API_KEY environment variable\n",
        "- [ ] Imported all necessary libraries including advanced RAG components\n",
        "- [ ] Created AdvancedRAGBackend class with configurable parameters\n",
        "- [ ] Implemented all required methods:\n",
        "  - [ ] `update_settings()` - Updates LLM and chunking parameters\n",
        "  - [ ] `initialize_database()` - Sets up vector database\n",
        "  - [ ] `get_postprocessor()` - Returns selected postprocessor\n",
        "  - [ ] `get_synthesizer()` - Returns selected synthesizer\n",
        "  - [ ] `advanced_query()` - Handles queries with all configuration options\n",
        "- [ ] Created advanced Gradio interface with all required components:\n",
        "  - [ ] Initialize database button\n",
        "  - [ ] Model selection dropdown (gpt-4o, gpt-4o-mini)\n",
        "  - [ ] Temperature slider (0 to 1, step 0.1)\n",
        "  - [ ] Chunk size input (default 512)\n",
        "  - [ ] Chunk overlap input (default 50)\n",
        "  - [ ] Similarity top-k slider (1 to 20, default 5)\n",
        "  - [ ] Node postprocessor multiselect\n",
        "  - [ ] Similarity cutoff slider (0.0 to 1.0, step 0.1, default 0.3)\n",
        "  - [ ] Response synthesizer dropdown\n",
        "  - [ ] Query input and submit button\n",
        "  - [ ] Response output\n",
        "  - [ ] Configuration display\n",
        "- [ ] Connected all components to backend functions\n",
        "- [ ] Successfully launched the application\n",
        "- [ ] Tested different parameter combinations\n",
        "- [ ] Verified all configuration options work correctly\n",
        "\n",
        "## ğŸŠ Congratulations!\n",
        "\n",
        "You've successfully built a professional, production-ready RAG application! You now have:\n",
        "\n",
        "- **Advanced Parameter Control**: Full control over all RAG system parameters\n",
        "- **Professional UI**: Clean, organized interface with proper layout\n",
        "- **Real-time Configuration**: Ability to experiment with different settings\n",
        "- **Production Patterns**: Understanding of how to build scalable AI applications\n",
        "\n",
        "## ğŸš€ Next Steps & Extensions\n",
        "\n",
        "**Potential Enhancements:**\n",
        "1. **Authentication**: Add user login and session management\n",
        "2. **Document Upload**: Allow users to upload their own documents\n",
        "3. **Chat History**: Implement conversation memory\n",
        "4. **Performance Monitoring**: Add response time and quality metrics\n",
        "5. **A/B Testing**: Compare different configurations side-by-side\n",
        "6. **Export Features**: Download responses and configurations\n",
        "7. **Advanced Visualizations**: Show document similarity scores and retrieval paths\n",
        "\n",
        "**Deployment Options:**\n",
        "- **Local**: Run on your machine for development\n",
        "- **Gradio Cloud**: Deploy with `interface.launch(share=True)`\n",
        "- **Hugging Face Spaces**: Deploy to Hugging Face for public access\n",
        "- **Docker**: Containerize for scalable deployment\n",
        "- **Cloud Platforms**: Deploy to AWS, GCP, or Azure\n",
        "\n",
        "You're now ready to build sophisticated AI-powered applications!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "accelerator",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}