{"cells":[{"cell_type":"markdown","metadata":{"id":"9sPXY_Us5N_-"},"source":["# Assignment 1: Vector Database Creation and Retrieval\n","## Day 6 Session 2 - RAG Fundamentals\n","\n","**OBJECTIVE:** Create a vector database from a folder of documents and implement basic retrieval functionality.\n","\n","**LEARNING GOALS:**\n","- Understand document loading with SimpleDirectoryReader\n","- Learn vector store setup with LanceDB\n","- Implement vector index creation\n","- Perform semantic search and retrieval\n","\n","**DATASET:** Use the data folder in `Day_6/session_2/data/` which contains multiple file types\n","\n","**INSTRUCTIONS:**\n","1. Complete each function by replacing the TODO comments with actual implementation\n","2. Run each cell after completing the function to test it\n","3. The answers can be found in the existing notebooks in the `llamaindex_rag/` folder\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive') #, force_remount=True)\n","\n","# ‚Äî‚Äî\n","# !pip install -q -r \"/content/requirements.txt\"\n","\n","# ‚Äî\n","# import os\n","# from getpass import getpass\n","\n","# # securely input your key\n","# os.environ[\"OPENROUTER_API_KEY\"] = getpass(\"Enter your OpenRouter key\")\n","# print(\"‚úì OpenrRouter key set successfully\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J_Ohfrfo7w8a","executionInfo":{"status":"ok","timestamp":1770532501883,"user_tz":360,"elapsed":1502,"user":{"displayName":"","userId":""}},"outputId":"91715e32-2dbe-4b08-cd3e-55a22a0a0c00"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## **We need to change the path for the installation requirement.txt for the below (it should be for the place where you have it in your drive):**"],"metadata":{"id":"FKX3XakU_Jhf"}},{"cell_type":"markdown","source":[],"metadata":{"id":"jDd8Og_2_GaT"}},{"cell_type":"code","source":["!pip install -q -r \"/content/drive/MyDrive/OutSkill RAG/requirements.txt\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bVYzMC7S-n_j","executionInfo":{"status":"ok","timestamp":1770533469704,"user_tz":360,"elapsed":8151,"user":{"displayName":"","userId":""}},"outputId":"35a2d583-32de-426f-c428-cbfe8c09a4ac"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: huggingface-hub 1.3.7 does not provide the extra 'inference'\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-g2xRpn5N__","executionInfo":{"status":"ok","timestamp":1770533233330,"user_tz":360,"elapsed":29441,"user":{"displayName":"","userId":""}},"outputId":"ae6d5512-593c-4dc2-c591-04ef1dc71ac7"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Libraries imported successfully!\n"]}],"source":["# Import required libraries\n","import os\n","from pathlib import Path\n","from typing import List\n","from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n","from llama_index.vector_stores.lancedb import LanceDBVectorStore\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","\n","print(\"‚úÖ Libraries imported successfully!\")"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245,"referenced_widgets":["0376b6fd58fe4290a3e1b30196d4d62d","f89f0e1d99c845548e2d077e51c58ba8","c98979fb2cc84b17b13ae4bd416d014a","d4d5abae223645d8b57d1f4eac4e82ee","af759e1b044f4fc481140639fb6056c5","ea39513921104ddc9eefe7945696cc4a","ed55537de57040b69d4ba60e00c9933c","94c47bd6b5de41b19ca761716690164b","2c35c0188fe341ccbefbef7bbf9ee7a4","c3458a14bbe64783be0673130518d04a","c0329f986e52490f87697111fb8f7c15"]},"id":"vZN5V5hV5N__","executionInfo":{"status":"ok","timestamp":1770534215517,"user_tz":360,"elapsed":1842,"user":{"displayName":"","userId":""}},"outputId":"418dcc36-5931-42a9-fa2d-a8c237851a48"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚ÑπÔ∏è  OPENROUTER_API_KEY not found - that's OK for this assignment!\n","   This assignment only uses local embeddings for vector operations.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0376b6fd58fe4290a3e1b30196d4d62d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["BertModel LOAD REPORT from: BAAI/bge-small-en-v1.5\n","Key                     | Status     |  | \n","------------------------+------------+--+-\n","embeddings.position_ids | UNEXPECTED |  | \n","\n","Notes:\n","- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ LlamaIndex configured with local embeddings\n","   Using BAAI/bge-small-en-v1.5 for document embeddings\n"]}],"source":["# Configure LlamaIndex Settings (Using OpenRouter - No OpenAI API Key needed)\n","def setup_llamaindex_settings():\n","    \"\"\"\n","    Configure LlamaIndex with local embeddings and OpenRouter for LLM.\n","    This assignment focuses on vector database operations, so we'll use local embeddings only.\n","    \"\"\"\n","    # Check for OpenRouter API key (for future use, not needed for this basic assignment)\n","    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n","    if not api_key:\n","        print(\"‚ÑπÔ∏è  OPENROUTER_API_KEY not found - that's OK for this assignment!\")\n","        print(\"   This assignment only uses local embeddings for vector operations.\")\n","\n","    # Configure local embeddings (no API key required)\n","    Settings.embed_model = HuggingFaceEmbedding(\n","        model_name=\"BAAI/bge-small-en-v1.5\",\n","        trust_remote_code=True\n","    )\n","\n","    print(\"‚úÖ LlamaIndex configured with local embeddings\")\n","    print(\"   Using BAAI/bge-small-en-v1.5 for document embeddings\")\n","\n","# Setup the configuration\n","setup_llamaindex_settings()\n"]},{"cell_type":"markdown","metadata":{"id":"bweMv39s5OAA"},"source":["## 1. Document Loading Function\n","\n","Complete the function below to load documents from a folder using `SimpleDirectoryReader`.\n","\n","**Note:** This assignment uses local embeddings only - no OpenAI API key required! We're configured to use OpenRouter for future LLM operations.\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UJapncXd5OAA","executionInfo":{"status":"ok","timestamp":1770534317784,"user_tz":360,"elapsed":44369,"user":{"displayName":"","userId":""}},"outputId":"9ac64349-16ed-424b-c049-650f0e0035d0"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]},{"output_type":"stream","name":"stdout","text":["Loaded 42 documents\n"]}],"source":["def load_documents_from_folder(folder_path: str):\n","    \"\"\"\n","    Load documents from a folder using SimpleDirectoryReader.\n","\n","    TODO: Complete this function to load documents from the given folder path.\n","    HINT: Use SimpleDirectoryReader with recursive parameter to load all files\n","\n","    Args:\n","        folder_path (str): Path to the folder containing documents\n","\n","    Returns:\n","        List of documents loaded from the folder\n","    \"\"\"\n","    # TODO: Create SimpleDirectoryReader instance\n","    reader = SimpleDirectoryReader(input_dir=folder_path, recursive=True)\n","\n","    # TODO: Load and return documents\n","    documents = reader.load_data()\n","\n","    return documents\n","\n","# Test the function after you complete it\n","test_folder = \"/content/drive/MyDrive/OutSkill RAG/data\"\n","documents = load_documents_from_folder(test_folder)\n","print(f\"Loaded {len(documents)} documents\")"]},{"cell_type":"markdown","metadata":{"id":"AwfWlRpU5OAA"},"source":["## 2. Vector Store Creation Function\n","\n","Complete the function below to create a LanceDB vector store.\n"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q45Ds-q-5OAA","executionInfo":{"status":"ok","timestamp":1770536530751,"user_tz":360,"elapsed":65,"user":{"displayName":"","userId":""}},"outputId":"986acb42-e4d0-4253-d2de-1d6d0514bd89"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:llama_index.vector_stores.lancedb.base:Table documents doesn't exist yet. Please add some data to create it.\n"]},{"output_type":"stream","name":"stdout","text":["Vector store created: True\n"]}],"source":["def create_vector_store(db_path: str = \"./vectordb\", table_name: str = \"documents\"):\n","    \"\"\"\n","    Create a LanceDB vector store for storing document embeddings.\n","\n","    TODO: Complete this function to create and configure a LanceDB vector store.\n","    HINT: Use LanceDBVectorStore with uri and table_name parameters\n","\n","    Args:\n","        db_path (str): Path where the vector database will be stored\n","        table_name (str): Name of the table in the vector database\n","\n","    Returns:\n","        LanceDBVectorStore: Configured vector store\n","    \"\"\"\n","    # TODO: Create the directory if it doesn't exist\n","    Path(db_path).mkdir(parents=True, exist_ok=True)\n","\n","    # TODO: Create vector store\n","    vector_store = LanceDBVectorStore(uri=db_path, table_name=table_name)\n","\n","    return vector_store\n","\n","# Test the function after you complete it\n","vector_store = create_vector_store(\"./assignment_vectordb\")\n","print(f\"Vector store created: {vector_store is not None}\")"]},{"cell_type":"markdown","metadata":{"id":"yrGC6MYP5OAA"},"source":["## 3. Vector Index Creation Function\n","\n","Complete the function below to create a vector index from documents.\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uWV54Zm15OAA","executionInfo":{"status":"ok","timestamp":1770535495004,"user_tz":360,"elapsed":35665,"user":{"displayName":"","userId":""}},"outputId":"003c3284-5da3-42cc-c1d3-0a16895ce9ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vector index created: True\n"]}],"source":["def create_vector_index(documents: List, vector_store):\n","    \"\"\"\n","    Create a vector index from documents using the provided vector store.\n","\n","    TODO: Complete this function to create a VectorStoreIndex from documents.\n","    HINT: Create StorageContext with vector_store, then use VectorStoreIndex.from_documents()\n","\n","    Args:\n","        documents: List of documents to index\n","        vector_store: LanceDB vector store to use for storage\n","\n","    Returns:\n","        VectorStoreIndex: The created vector index\n","    \"\"\"\n","    # TODO: Create storage context with vector store\n","    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n","\n","    # TODO: Create index from documents\n","    index = VectorStoreIndex.from_documents(\n","        documents,\n","        storage_context=storage_context,\n","    )\n","\n","    return index\n","\n","# Test the function after you complete it (will only work after previous functions are completed)\n","if documents and vector_store:\n","    index = create_vector_index(documents, vector_store)\n","    print(f\"Vector index created: {index is not None}\")\n","else:\n","    print(\"Complete previous functions first to test this one\")"]},{"cell_type":"markdown","metadata":{"id":"Vc3t_zRx5OAA"},"source":["## 4. Document Search Function\n","\n","Complete the function below to search for relevant documents using the vector index.\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fpc1yjQu5OAA","executionInfo":{"status":"ok","timestamp":1770535594693,"user_tz":360,"elapsed":185,"user":{"displayName":"","userId":""}},"outputId":"b6b8557d-b801-41c1-d202-4decd0efa09c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2 results for query: 'What are AI agents?'\n","Result 1: task. In single agent patterns there is no feedback mechanism from other AI agents; however, there m...\n","Result 2: agent-personas or the user is not needed, multi-agent architectures tend to thrive more when collabo...\n"]}],"source":["def search_documents(index, query: str, top_k: int = 3):\n","    \"\"\"\n","    Search for relevant documents using the vector index.\n","\n","    TODO: Complete this function to perform semantic search on the index.\n","    HINT: Use index.as_retriever() with similarity_top_k parameter, then retrieve(query)\n","\n","    Args:\n","        index: Vector index to search\n","        query (str): Search query\n","        top_k (int): Number of top results to return\n","\n","    Returns:\n","        List of retrieved document nodes\n","    \"\"\"\n","    # TODO: Create retriever from index\n","    retriever = index.as_retriever(similarity_top_k=top_k)\n","\n","    # TODO: Retrieve documents for the query\n","    results = retriever.retrieve(query)\n","\n","    return results\n","\n","# Test the function after you complete it (will only work after all previous functions are completed)\n","if 'index' in locals() and index is not None:\n","    test_query = \"What are AI agents?\"\n","    results = search_documents(index, test_query, top_k=2)\n","    print(f\"Found {len(results)} results for query: '{test_query}'\")\n","    for i, result in enumerate(results, 1):\n","        print(f\"Result {i}: {result.text[:100] if hasattr(result, 'text') else 'No text'}...\")\n","else:\n","    print(\"Complete all previous functions first to test this one\")"]},{"cell_type":"markdown","metadata":{"id":"IYgHXNV05OAA"},"source":["## 5. Final Test - Complete Pipeline\n","\n","Once you've completed all the functions above, run this cell to test the complete pipeline with multiple search queries.\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kNHE8HJ45OAA","executionInfo":{"status":"ok","timestamp":1770536264202,"user_tz":360,"elapsed":78616,"user":{"displayName":"","userId":""}},"outputId":"c584fbcf-1951-4068-ba08-98d4c2f60c52"},"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Testing Complete Vector Database Pipeline\n","==================================================\n","\n","üìÇ Step 1: Loading documents...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]},{"output_type":"stream","name":"stdout","text":["   Loaded 42 documents\n","\n","üóÑÔ∏è Step 2: Creating vector store...\n","   Vector store status: ‚úÖ Created\n","\n","üîó Step 3: Creating vector index...\n","   Index status: ‚úÖ Created\n","\n","üîç Step 4: Testing search functionality...\n","\n","   üîé Query: 'What are AI agents?'\n","      1. task. In single agent patterns there is no feedback mechanism from other AI agents; however, there m... (Score: 0.6221)\n","\n","   üîé Query: 'How to evaluate agent performance?'\n","      1. steps, but the answers are limited to Yes/No responses [7]. As the industry continues to pivot towar... (Score: 0.6774)\n","\n","   üîé Query: 'Italian recipes and cooking'\n","      1. # üçù Classic Spaghetti Carbonara Recipe\n","\n","## Ingredients\n","- 400g spaghetti pasta\n","- 4 large egg yolks\n","- ... (Score: 0.6227)\n","\n","   üîé Query: 'Financial analysis and investment'\n","      1. Stock, AAPL, Apple Inc, 10000, 12500, 25.0, Medium\n","Stock, GOOGL, Alphabet Inc, 8000, 9200, 15.0, Med... (Score: 0.5639)\n","\n","==================================================\n","üéØ Assignment Status:\n","   Documents loaded: ‚úÖ\n","   Vector store created: ‚úÖ\n","   Index created: ‚úÖ\n","   Search working: ‚úÖ\n","\n","üéâ Congratulations! You've successfully completed the assignment!\n","   You've built a complete vector database with search functionality!\n"]}],"source":["# Final test of the complete pipeline\n","print(\"üöÄ Testing Complete Vector Database Pipeline\")\n","print(\"=\" * 50)\n","\n","# Re-run the complete pipeline to ensure everything works\n","data_folder = \"/content/drive/MyDrive/OutSkill RAG/data\"\n","vector_db_path = \"./assignment_vectordb\"\n","\n","# Step 1: Load documents\n","print(\"\\nüìÇ Step 1: Loading documents...\")\n","documents = load_documents_from_folder(data_folder)\n","print(f\"   Loaded {len(documents)} documents\")\n","\n","# Step 2: Create vector store\n","print(\"\\nüóÑÔ∏è Step 2: Creating vector store...\")\n","vector_store = create_vector_store(vector_db_path)\n","print(\"   Vector store status:\", \"‚úÖ Created\" if vector_store else \"‚ùå Failed\")\n","\n","# Step 3: Create vector index\n","print(\"\\nüîó Step 3: Creating vector index...\")\n","if documents and vector_store:\n","    index = create_vector_index(documents, vector_store)\n","    print(\"   Index status:\", \"‚úÖ Created\" if index else \"‚ùå Failed\")\n","else:\n","    index = None\n","    print(\"   ‚ùå Cannot create index - missing documents or vector store\")\n","\n","# Step 4: Test multiple search queries\n","print(\"\\nüîç Step 4: Testing search functionality...\")\n","if index:\n","    search_queries = [\n","        \"What are AI agents?\",\n","        \"How to evaluate agent performance?\",\n","        \"Italian recipes and cooking\",\n","        \"Financial analysis and investment\"\n","    ]\n","\n","    for query in search_queries:\n","        print(f\"\\n   üîé Query: '{query}'\")\n","        results = search_documents(index, query, top_k=2)\n","\n","        if results:\n","            for i, result in enumerate(results, 1):\n","                text_preview = result.text[:100] if hasattr(result, 'text') else \"No text available\"\n","                score = f\" (Score: {result.score:.4f})\" if hasattr(result, 'score') else \"\"\n","                print(f\"      {i}. {text_preview}...{score}\")\n","        else:\n","            print(\"      No results found\")\n","else:\n","    print(\"   ‚ùå Cannot test search - index not created\")\n","\n","print(\"\\n\" + \"=\" * 50)\n","print(\"üéØ Assignment Status:\")\n","print(f\"   Documents loaded: {'‚úÖ' if documents else '‚ùå'}\")\n","print(f\"   Vector store created: {'‚úÖ' if vector_store else '‚ùå'}\")\n","print(f\"   Index created: {'‚úÖ' if index else '‚ùå'}\")\n","print(f\"   Search working: {'‚úÖ' if index else '‚ùå'}\")\n","\n","if documents and vector_store and index:\n","    print(\"\\nüéâ Congratulations! You've successfully completed the assignment!\")\n","    print(\"   You've built a complete vector database with search functionality!\")\n","else:\n","    print(\"\\nüìù Please complete the TODO functions above to finish the assignment.\")\n"]}],"metadata":{"kernelspec":{"display_name":"accelerator","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"colab":{"provenance":[{"file_id":"https://github.com/eng-accelerator/ai-accelerator-C4/blob/main/Day_7/assignments/assignment_1_vector_db_basics.ipynb","timestamp":1770543137375}]},"widgets":{"application/vnd.jupyter.widget-state+json":{"0376b6fd58fe4290a3e1b30196d4d62d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f89f0e1d99c845548e2d077e51c58ba8","IPY_MODEL_c98979fb2cc84b17b13ae4bd416d014a","IPY_MODEL_d4d5abae223645d8b57d1f4eac4e82ee"],"layout":"IPY_MODEL_af759e1b044f4fc481140639fb6056c5"}},"f89f0e1d99c845548e2d077e51c58ba8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea39513921104ddc9eefe7945696cc4a","placeholder":"‚Äã","style":"IPY_MODEL_ed55537de57040b69d4ba60e00c9933c","value":"Loading‚Äáweights:‚Äá100%"}},"c98979fb2cc84b17b13ae4bd416d014a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_94c47bd6b5de41b19ca761716690164b","max":199,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2c35c0188fe341ccbefbef7bbf9ee7a4","value":199}},"d4d5abae223645d8b57d1f4eac4e82ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3458a14bbe64783be0673130518d04a","placeholder":"‚Äã","style":"IPY_MODEL_c0329f986e52490f87697111fb8f7c15","value":"‚Äá199/199‚Äá[00:00&lt;00:00,‚Äá533.95it/s,‚ÄáMaterializing‚Äáparam=pooler.dense.weight]"}},"af759e1b044f4fc481140639fb6056c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea39513921104ddc9eefe7945696cc4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed55537de57040b69d4ba60e00c9933c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"94c47bd6b5de41b19ca761716690164b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c35c0188fe341ccbefbef7bbf9ee7a4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c3458a14bbe64783be0673130518d04a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0329f986e52490f87697111fb8f7c15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}