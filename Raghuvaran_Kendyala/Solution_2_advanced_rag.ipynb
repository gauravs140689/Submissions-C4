{"cells":[{"cell_type":"markdown","metadata":{"id":"jZT8hHuF5WWk"},"source":["# Assignment 2: Advanced RAG Techniques\n","## Day 6 Session 2 - Advanced RAG Fundamentals\n","\n","**OBJECTIVE:** Implement advanced RAG techniques including postprocessors, response synthesizers, and structured outputs.\n","\n","**LEARNING GOALS:**\n","- Understand and implement node postprocessors for filtering and reranking\n","- Learn different response synthesis strategies (TreeSummarize, Refine)\n","- Create structured outputs using Pydantic models\n","- Build advanced retrieval pipelines with multiple processing stages\n","\n","**DATASET:** Use the same data folder as Assignment 1 (`Day_6/session_2/data/`)\n","\n","**PREREQUISITES:** Complete Assignment 1 first\n","\n","**INSTRUCTIONS:**\n","1. Complete each function by replacing the TODO comments with actual implementation\n","2. Run each cell after completing the function to test it\n","3. The answers can be found in the `03_advanced_rag_techniques.ipynb` notebook\n","4. Each technique builds on the previous one\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yIIDXT27MZRQ","executionInfo":{"status":"ok","timestamp":1770536652968,"user_tz":360,"elapsed":18651,"user":{"displayName":"","userId":""}},"outputId":"021e1e64-fb90-48e8-ad25-686c9c0f146f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install -q -r \"/content/drive/MyDrive/OutSkill RAG/requirements.txt\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FyfpqY5ANXxw","executionInfo":{"status":"ok","timestamp":1770536748984,"user_tz":360,"elapsed":63187,"user":{"displayName":"","userId":""}},"outputId":"2681735d-32dd-4093-d56e-654071c4f56d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.2/803.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m266.2/803.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m798.7/803.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: huggingface-hub 1.3.7 does not provide the extra 'inference'\u001b[0m\u001b[33m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.8/96.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K2twB-Ya5WWl","executionInfo":{"status":"ok","timestamp":1770536791266,"user_tz":360,"elapsed":39020,"user":{"displayName":"","userId":""}},"outputId":"acb75e49-b155-4ddf-c99d-7fcd3347b064"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Advanced RAG libraries imported successfully!\n"]}],"source":["# Import required libraries for advanced RAG\n","import os\n","from pathlib import Path\n","from typing import Dict, List, Optional, Any\n","from pydantic import BaseModel, Field\n","\n","# Core LlamaIndex components\n","from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n","from llama_index.core.query_engine import RetrieverQueryEngine\n","from llama_index.core.retrievers import VectorIndexRetriever\n","\n","# Vector store\n","from llama_index.vector_stores.lancedb import LanceDBVectorStore\n","\n","# Embeddings and LLM\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.llms.openrouter import OpenRouter\n","\n","# Advanced RAG components (we'll use these in the assignments)\n","from llama_index.core.postprocessor import SimilarityPostprocessor\n","from llama_index.core.response_synthesizers import TreeSummarize, Refine, CompactAndRefine\n","from llama_index.core.output_parsers import PydanticOutputParser\n","\n","print(\"âœ… Advanced RAG libraries imported successfully!\")\n"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263,"referenced_widgets":["8f704a53123a4085b27b7a30369f9643","8c21b60c16ce4077b30f84e316086015","c32589679603455daf0e87f727b17b06","223b7615f2434098aa5f30c6e8ee46e6","3a7e9f3ed32e49b9a8352d838b231d32","4c1c0764df7a43359e771bda8de6547c","f5f8596d3c574a298b94a0937523410d","8afb3a752fe34611b7323a9c28609fda","e6db573d7d63413fa394ebbf7558f0d9","83bc837425bf4c888c3ea8c9999776ea","a122930c593448f28edccb357cbb077a"]},"id":"3TzrUpwZ5WWl","executionInfo":{"status":"ok","timestamp":1770538723812,"user_tz":360,"elapsed":1832,"user":{"displayName":"","userId":""}},"outputId":"381c059d-de97-475d-e03c-22be5d9bfb3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… OPENROUTER_API_KEY found - full advanced RAG functionality available\n"]},{"output_type":"display_data","data":{"text/plain":["Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f704a53123a4085b27b7a30369f9643"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["BertModel LOAD REPORT from: BAAI/bge-small-en-v1.5\n","Key                     | Status     |  | \n","------------------------+------------+--+-\n","embeddings.position_ids | UNEXPECTED |  | \n","\n","Notes:\n","- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Advanced RAG settings configured\n","   - Chunk size: 512 (optimized for precision)\n","   - Using local embeddings for cost efficiency\n","   - OpenRouter LLM ready for response synthesis\n"]}],"source":["# Configure Advanced RAG Settings (Using OpenRouter)\n","def setup_advanced_rag_settings():\n","    \"\"\"\n","    Configure LlamaIndex with optimized settings for advanced RAG.\n","    Uses local embeddings and OpenRouter for LLM operations.\n","    \"\"\"\n","    # Check for OpenRouter API key\n","    api_key = os.getenv(\"OPENROUTER_API_KEY\")\n","    if not api_key:\n","        print(\"âš ï¸  OPENROUTER_API_KEY not found - LLM operations will be limited\")\n","        print(\"   You can still complete postprocessor and retrieval exercises\")\n","    else:\n","        print(\"âœ… OPENROUTER_API_KEY found - full advanced RAG functionality available\")\n","\n","        # Configure OpenRouter LLM\n","        Settings.llm = OpenRouter(\n","            api_key=api_key,\n","            model=\"gpt-4o\",\n","            temperature=0.1  # Lower temperature for more consistent responses\n","        )\n","\n","    # Configure local embeddings (no API key required)\n","    Settings.embed_model = HuggingFaceEmbedding(\n","        model_name=\"BAAI/bge-small-en-v1.5\",\n","        trust_remote_code=True\n","    )\n","\n","    # Advanced RAG configuration\n","    Settings.chunk_size = 512  # Smaller chunks for better precision\n","    Settings.chunk_overlap = 50\n","\n","    print(\"âœ… Advanced RAG settings configured\")\n","    print(\"   - Chunk size: 512 (optimized for precision)\")\n","    print(\"   - Using local embeddings for cost efficiency\")\n","    print(\"   - OpenRouter LLM ready for response synthesis\")\n","\n","# Setup the configuration\n","setup_advanced_rag_settings()\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"df7283c0","executionInfo":{"status":"ok","timestamp":1770538729530,"user_tz":360,"elapsed":236,"user":{"displayName":"","userId":""}},"outputId":"83fb25e2-12a4-46b6-ad41-17fa12b0e9a0"},"source":["import os\n","from google.colab import userdata\n","\n","# Retrieve the API key from Colab secrets and set it as an environment variable\n","os.environ[\"OPENROUTER_API_KEY\"] = userdata.get(\"OPENROUTER_API_KEY\")\n","print(\"âœ… OPENROUTER_API_KEY loaded from Colab secrets and set as environment variable.\")"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… OPENROUTER_API_KEY loaded from Colab secrets and set as environment variable.\n"]}]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":260,"referenced_widgets":["019b60bb634341a391050c111d3ecfd8","456cbc890e5a42b19d7ba209d02e2efb","e95be18aed3c4361a58fe84559f2796a","f17aa823794d49dba9068456e1a3cff3","830988ccbaae4c2782324bf2ea5727e5","101102c483fc420d833a47e1a93edac4","4df1f1c7f77d4027a442cffa39d0a7a6","4ebbd28fa2b349cbb6c3605037cdefb6","bcb86af265ee4da18c429846171b26be","afb7f3a4967e4b9299387651b3dd39e8","32dd122e559740969ed5433b1a142b20","bf82b8506fe646c496b9637f4525f9fc","9d0d75a276894f1893e8fd9f1250093f","1d7fd944f9f443dba162bd92b1628239","257fc9e289c64a2baa164d7f3a5ab87b","35137caaf2fe452fa67c01cd26f6dbe7","b6dcf66104c64b9d8836f48aefee7072","335c0500a6e249d095f7e32cadd1e42b","02c44a29522b4722adfe7d4338d25865","24b30542f03947c28df05c4415fe9ea6","2f5f8fc7f0cb4b0597911cfa6e63f84f","a42c7d71a2d94cac87f9d198fb92664f"]},"id":"gZbaw6oZ5WWl","executionInfo":{"status":"ok","timestamp":1770538822204,"user_tz":360,"elapsed":86769,"user":{"displayName":"","userId":""}},"outputId":"9b7e1337-ef5f-4e1a-f280-80bc582251df"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ“ Setting up basic index for advanced RAG...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]},{"output_type":"display_data","data":{"text/plain":["Parsing nodes:   0%|          | 0/42 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"019b60bb634341a391050c111d3ecfd8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating embeddings:   0%|          | 0/90 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf82b8506fe646c496b9637f4525f9fc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ… Basic index created with 42 documents\n","   Ready for advanced RAG techniques!\n","ğŸš€ Ready to implement advanced RAG techniques!\n"]}],"source":["# Setup: Create index from Assignment 1 (reuse the basic functionality)\n","def setup_basic_index(data_folder: str = \"/content/drive/MyDrive/OutSkill RAG/data\", force_rebuild: bool = False):\n","    \"\"\"\n","    Create a basic vector index that we'll enhance with advanced techniques.\n","    This reuses the concepts from Assignment 1.\n","    \"\"\"\n","    # Create vector store\n","    vector_store = LanceDBVectorStore(\n","        uri=\"./advanced_rag_vectordb\",\n","        table_name=\"documents\"\n","    )\n","\n","    # Load documents\n","    if not Path(data_folder).exists():\n","        print(f\"âŒ Data folder not found: {data_folder}\")\n","        return None\n","\n","    reader = SimpleDirectoryReader(input_dir=data_folder, recursive=True)\n","    documents = reader.load_data()\n","\n","    # Create storage context and index\n","    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n","    index = VectorStoreIndex.from_documents(\n","        documents,\n","        storage_context=storage_context,\n","        show_progress=True\n","    )\n","\n","    print(f\"âœ… Basic index created with {len(documents)} documents\")\n","    print(\"   Ready for advanced RAG techniques!\")\n","    return index\n","\n","# Create the basic index\n","print(\"ğŸ“ Setting up basic index for advanced RAG...\")\n","index = setup_basic_index()\n","\n","if index:\n","    print(\"ğŸš€ Ready to implement advanced RAG techniques!\")\n","else:\n","    print(\"âŒ Failed to create index - check data folder path\")\n"]},{"cell_type":"markdown","metadata":{"id":"XMjHLJx95WWm"},"source":["## 1. Node Postprocessors - Similarity Filtering\n","\n","**Concept:** Postprocessors refine retrieval results after the initial vector search. The `SimilarityPostprocessor` filters out chunks that fall below a relevance threshold.\n","\n","**Why it matters:** Raw vector search often returns some irrelevant results. Filtering improves precision and response quality.\n","\n","Complete the function below to create a query engine with similarity filtering.\n"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0CZNeuVZ5WWm","executionInfo":{"status":"ok","timestamp":1770538825123,"user_tz":360,"elapsed":2902,"user":{"displayName":"","userId":""}},"outputId":"ca808b2c-3ef7-4097-8b02-ee819f2b5fa2"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Query engine with similarity filtering created\n","\n","ğŸ” Testing query: 'What are the benefits of AI agents?'\n","ğŸ“ Response: AI agents offer several benefits, including enhanced effectiveness across various benchmarks and problem types. They are capable of achieving complex goals through improved reasoning, planning, and tool execution capabilities. Multi-agent architectures, in particular, are beneficial for tasks requiring feedback from multiple personas and for parallelizing distinct tasks or workflows. Additionally, AI agents can operate dynamically, allowing for more flexible task management and execution. They also benefit from iterative feedback and refinement, which helps in solving complex problems more effectively.\n","   (Complete the function above to test the response)\n"]}],"source":["def create_query_engine_with_similarity_filter(index, similarity_cutoff: float = 0.3, top_k: int = 10):\n","    \"\"\"\n","    Create a query engine that filters results based on similarity scores.\n","\n","    TODO: Complete this function to create a query engine with similarity postprocessing.\n","    HINT: Use index.as_query_engine() with node_postprocessors parameter containing SimilarityPostprocessor\n","\n","    Args:\n","        index: Vector index to query\n","        similarity_cutoff: Minimum similarity score (0.0 to 1.0)\n","        top_k: Number of initial results to retrieve before filtering\n","\n","    Returns:\n","        Query engine with similarity filtering\n","    \"\"\"\n","    # TODO: Create similarity postprocessor with the cutoff threshold\n","    similarity_processor = SimilarityPostprocessor(\n","        similarity_cutoff= similarity_cutoff\n","    )\n","\n","    # TODO: Create query engine with similarity filtering\n","    query_engine = index.as_query_engine(similarity_top_k=top_k, node_postprocessors=[similarity_processor])\n","\n","    return query_engine\n","\n","    # PLACEHOLDER - Replace with actual implementation\n","    #print(f\"TODO: Create query engine with similarity cutoff {similarity_cutoff}\")\n","    #return None\n","\n","# Test the function\n","if index:\n","    filtered_engine = create_query_engine_with_similarity_filter(index, similarity_cutoff=0.3)\n","\n","    if filtered_engine:\n","        print(\"âœ… Query engine with similarity filtering created\")\n","\n","        # Test query\n","        test_query = \"What are the benefits of AI agents?\"\n","        print(f\"\\nğŸ” Testing query: '{test_query}'\")\n","\n","        # Uncomment when implemented:\n","        response = filtered_engine.query(test_query)\n","        print(f\"ğŸ“ Response: {response}\")\n","        print(\"   (Complete the function above to test the response)\")\n","    else:\n","        print(\"âŒ Failed to create filtered query engine\")\n","else:\n","    print(\"âŒ No index available - run previous cells first\")\n"]},{"cell_type":"markdown","metadata":{"id":"LuzJj6fD5WWm"},"source":["## 2. Response Synthesizers - TreeSummarize\n","\n","**Concept:** Response synthesizers control how retrieved information becomes final answers. `TreeSummarize` builds responses hierarchically, ideal for complex analytical questions.\n","\n","**Why it matters:** Different synthesis strategies work better for different query types. TreeSummarize excels at comprehensive analysis and long-form responses.\n","\n","Complete the function below to create a query engine with TreeSummarize response synthesis.\n"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4E6iNqb45WWm","executionInfo":{"status":"ok","timestamp":1770539337381,"user_tz":360,"elapsed":1799,"user":{"displayName":"","userId":""}},"outputId":"8b97ba95-39ff-46e6-9772-79a95be85047"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Query engine with TreeSummarize created\n","\n","ğŸ” Testing analytical query: 'Compare the advantages and disadvantages of different AI agent frameworks'\n","ğŸ“ TreeSummarize Response:\n","Different AI agent frameworks offer various advantages and disadvantages. On the positive side, architectures that incorporate advanced techniques tend to perform better across diverse benchmarks and problem types. This suggests they are more adaptable and effective in handling a range of tasks. However, there are also notable challenges associated with these frameworks. Key issues include the need for comprehensive benchmarks to evaluate agent performance, ensuring real-world applicability, and addressing biases inherent in language models. These challenges highlight areas where current frameworks may fall short and where future improvements are necessary to develop more reliable and effective AI agents.\n","   (Complete the function above to test comprehensive analysis)\n"]}],"source":["def create_query_engine_with_tree_summarize(index, top_k: int = 5):\n","    \"\"\"\n","    Create a query engine that uses TreeSummarize for comprehensive responses.\n","\n","    TODO: Complete this function to create a query engine with TreeSummarize synthesis.\n","    HINT: Create a TreeSummarize instance, then use index.as_query_engine() with response_synthesizer parameter\n","\n","    Args:\n","        index: Vector index to query\n","        top_k: Number of results to retrieve\n","\n","    Returns:\n","        Query engine with TreeSummarize synthesis\n","    \"\"\"\n","    # TODO: Create TreeSummarize response synthesizer\n","    tree_synthesizer = TreeSummarize()\n","\n","    # TODO: Create query engine with the synthesizer\n","    query_engine = index.as_query_engine(response_synthesizer=tree_synthesizer)\n","\n","    return query_engine\n","\n","    # PLACEHOLDER - Replace with actual implementation\n","    # print(f\"TODO: Create query engine with TreeSummarize synthesis\")\n","    # return None\n","\n","# Test the function\n","if index:\n","    tree_engine = create_query_engine_with_tree_summarize(index)\n","\n","    if tree_engine:\n","        print(\"âœ… Query engine with TreeSummarize created\")\n","\n","        # Test with a complex analytical query\n","        analytical_query = \"Compare the advantages and disadvantages of different AI agent frameworks\"\n","        print(f\"\\nğŸ” Testing analytical query: '{analytical_query}'\")\n","\n","        # Uncomment when implemented:\n","        response = tree_engine.query(analytical_query)\n","        print(f\"ğŸ“ TreeSummarize Response:\\n{response}\")\n","        print(\"   (Complete the function above to test comprehensive analysis)\")\n","    else:\n","        print(\"âŒ Failed to create TreeSummarize query engine\")\n","else:\n","    print(\"âŒ No index available - run previous cells first\")\n"]},{"cell_type":"markdown","metadata":{"id":"9xpEpOiI5WWm"},"source":["## 3. Structured Outputs with Pydantic Models\n","\n","**Concept:** Structured outputs ensure predictable, parseable responses using Pydantic models. This is essential for API endpoints and data pipelines.\n","\n","**Why it matters:** Instead of free-text responses, you get type-safe, validated data structures that applications can reliably process.\n","\n","Complete the function below to create a structured output system for extracting research paper information.\n"]},{"cell_type":"code","source":["# First, define the Pydantic models for structured outputs\n","class ResearchPaperInfo(BaseModel):\n","    \"\"\"Structured information about a research paper or AI concept.\"\"\"\n","    title: str = Field(description=\"The main title or concept name\")\n","    key_points: List[str] = Field(description=\"3-5 main points or findings\")\n","    applications: List[str] = Field(description=\"Practical applications or use cases\")\n","    summary: str = Field(description=\"Brief 2-3 sentence summary\")\n","\n","# Import the missing component\n","from llama_index.core.program import LLMTextCompletionProgram\n","\n","def create_structured_output_program(output_model: BaseModel = ResearchPaperInfo):\n","    \"\"\"\n","    Create a structured output program using Pydantic models.\n","\n","    TODO: Complete this function to create a structured output program.\n","    HINT: Use LLMTextCompletionProgram.from_defaults() with PydanticOutputParser and a prompt template\n","\n","    Args:\n","        output_model: Pydantic model class for structured output\n","\n","    Returns:\n","        LLMTextCompletionProgram that returns structured data\n","    \"\"\"\n","    # TODO: Create output parser with the Pydantic model\n","    output_parser = PydanticOutputParser(output_model)\n","\n","    # TODO: Create the structured output program\n","    program = LLMTextCompletionProgram.from_defaults(\n","        output_parser=output_parser,\n","        prompt_template_str=(\"Output your response as a JSON object, strictly following the following schema: {schema}\" ))\n","\n","    return program\n","\n","    # PLACEHOLDER - Replace with actual implementation\n","    # print(f\"TODO: Create structured output program with {output_model.__name__}\")\n","    # return None\n","\n","# Test the function\n","if index:\n","    structured_program = create_structured_output_program(ResearchPaperInfo)\n","\n","    if structured_program:\n","        print(\"âœ… Structured output program created\")\n","\n","        # Test with retrieval and structured extraction\n","        structure_query = \"Tell me about AI agents and their capabilities\"\n","        print(f\"\\nğŸ” Testing structured query: '{structure_query}'\")\n","\n","        # Get context for structured extraction (Uncomment when implemented)\n","        retriever = VectorIndexRetriever(index=index, similarity_top_k=3)\n","        nodes = retriever.retrieve(structure_query)\n","        context = \"\\n\".join([node.text for node in nodes])\n","\n","        # Uncomment when implemented:\n","        response = structured_program(context=context, query=structure_query)\n","        print(f\"ğŸ“Š Structured Response:\\n{response}\")\n","        print(\"   (Complete the function above to get structured JSON output)\")\n","\n","        print(\"\\nğŸ’¡ Expected output format:\")\n","        print(\"   - title: String\")\n","        print(\"   - key_points: List of strings\")\n","        print(\"   - applications: List of strings\")\n","        print(\"   - summary: String\")\n","    else:\n","        print(\"âŒ Failed to create structured output program\")\n","else:\n","    print(\"âŒ No index available - run previous cells first\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4AjelARiaJ1C","executionInfo":{"status":"ok","timestamp":1770540314974,"user_tz":360,"elapsed":2362,"user":{"displayName":"","userId":""}},"outputId":"df79812a-cc43-491c-bac6-40aa46d44f5d"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Structured output program created\n","\n","ğŸ” Testing structured query: 'Tell me about AI agents and their capabilities'\n","ğŸ“Š Structured Response:\n","title='Transformers in Natural Language Processing' key_points=['Transformers utilize self-attention mechanisms to process input data.', 'They have significantly improved the performance of NLP tasks such as translation and summarization.', 'Transformers allow for parallelization, making them more efficient than RNNs.', 'The architecture is the foundation for models like BERT and GPT.', 'They can handle long-range dependencies in text data.'] applications=['Machine translation', 'Text summarization', 'Sentiment analysis', 'Question answering systems', 'Chatbots and conversational agents'] summary='Transformers have revolutionized natural language processing by introducing self-attention mechanisms that improve efficiency and performance in various tasks. They serve as the backbone for advanced models like BERT and GPT, enabling applications in translation, summarization, and more.'\n","   (Complete the function above to get structured JSON output)\n","\n","ğŸ’¡ Expected output format:\n","   - title: String\n","   - key_points: List of strings\n","   - applications: List of strings\n","   - summary: String\n"]}]},{"cell_type":"markdown","metadata":{"id":"nCUU5ZLC5WWm"},"source":["## 4. Advanced Pipeline - Combining All Techniques\n","\n","**Concept:** Combine multiple advanced techniques into a single powerful query engine: similarity filtering + response synthesis + structured output.\n","\n","**Why it matters:** Production RAG systems often need multiple techniques working together for optimal results.\n","\n","Complete the function below to create a comprehensive advanced RAG pipeline.\n"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-B1wxkmP5WWm","executionInfo":{"status":"ok","timestamp":1770540539096,"user_tz":360,"elapsed":3482,"user":{"displayName":"","userId":""}},"outputId":"d2cc5a6c-eec7-472e-a376-d4bfaa0295f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Advanced RAG pipeline created successfully!\n","   ğŸ”§ Similarity filtering: âœ…\n","   ğŸŒ³ TreeSummarize synthesis: âœ…\n","\n","ğŸ” Testing complex query: 'Analyze the current state and future potential of AI agent technologies'\n","ğŸš€ Advanced RAG Response:\n","The current state of AI agent technologies is promising, with advancements in their ability to achieve complex goals through enhanced reasoning, planning, and tool execution capabilities. These technologies are being developed to be more effective across various benchmarks and problem types. However, there are notable limitations that need addressing, such as comprehensive agent benchmarks, real-world applicability, and the mitigation of harmful language model biases. \n","\n","Future potential lies in overcoming these challenges to enable more reliable agents. The progression from static language models to dynamic, autonomous agents is a key area of focus, with research exploring both single-agent and multi-agent architectures. These architectures aim to enhance language models by enabling them to execute goals on behalf of or alongside human users. Key themes for future development include clear feedback mechanisms, task decomposition, iterative refinement, and role definition, which are crucial for improving agent performance. Additionally, frameworks like Agno and CrewAI are facilitating the rapid development and deployment of robust agentic AI systems, particularly in fields like finance. Overall, the future of AI agent technologies looks towards more autonomous, efficient, and reliable systems.\n","   (Complete the function above to test the full pipeline)\n","\n","ğŸ¯ This should provide:\n","   - Filtered relevant results only\n","   - Comprehensive analytical response\n","   - Combined postprocessing and synthesis\n"]}],"source":["def create_advanced_rag_pipeline(index, similarity_cutoff: float = 0.3, top_k: int = 10):\n","    \"\"\"\n","    Create a comprehensive advanced RAG pipeline combining multiple techniques.\n","\n","    TODO: Complete this function to create the ultimate advanced RAG query engine.\n","    HINT: Combine SimilarityPostprocessor + TreeSummarize using index.as_query_engine()\n","\n","    Args:\n","        index: Vector index to query\n","        similarity_cutoff: Minimum similarity score for filtering\n","        top_k: Number of initial results to retrieve\n","\n","    Returns:\n","        Advanced query engine with filtering and synthesis combined\n","    \"\"\"\n","    # TODO: Create similarity postprocessor\n","    similarity_processor = SimilarityPostprocessor(\n","        similarity_cutoff= similarity_cutoff\n","    )\n","\n","    # TODO: Create TreeSummarize for comprehensive responses\n","    tree_synthesizer = TreeSummarize()\n","\n","    # TODO: Create the comprehensive query engine combining both techniques\n","    advanced_engine = index.as_query_engine(similarity_top_k=top_k, node_postprocessors=[similarity_processor],response_synthesizer=tree_synthesizer)\n","\n","    return advanced_engine\n","\n","    # PLACEHOLDER - Replace with actual implementation\n","    # print(f\"TODO: Create advanced RAG pipeline with all techniques\")\n","    # return None\n","\n","# Test the comprehensive pipeline\n","if index:\n","    advanced_pipeline = create_advanced_rag_pipeline(index)\n","\n","    if advanced_pipeline:\n","        print(\"âœ… Advanced RAG pipeline created successfully!\")\n","        print(\"   ğŸ”§ Similarity filtering: âœ…\")\n","        print(\"   ğŸŒ³ TreeSummarize synthesis: âœ…\")\n","\n","        # Test with complex query\n","        complex_query = \"Analyze the current state and future potential of AI agent technologies\"\n","        print(f\"\\nğŸ” Testing complex query: '{complex_query}'\")\n","\n","        # Uncomment when implemented:\n","        response = advanced_pipeline.query(complex_query)\n","        print(f\"ğŸš€ Advanced RAG Response:\\n{response}\")\n","        print(\"   (Complete the function above to test the full pipeline)\")\n","\n","        print(\"\\nğŸ¯ This should provide:\")\n","        print(\"   - Filtered relevant results only\")\n","        print(\"   - Comprehensive analytical response\")\n","        print(\"   - Combined postprocessing and synthesis\")\n","    else:\n","        print(\"âŒ Failed to create advanced RAG pipeline\")\n","else:\n","    print(\"âŒ No index available - run previous cells first\")\n"]},{"cell_type":"markdown","metadata":{"id":"PT2FrQKK5WWn"},"source":["## 5. Final Test - Compare Basic vs Advanced RAG\n","\n","Once you've completed all the functions above, run this cell to compare basic RAG with your advanced techniques.\n"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nz5JumQE5WWn","executionInfo":{"status":"ok","timestamp":1770540836535,"user_tz":360,"elapsed":49,"user":{"displayName":"","userId":""}},"outputId":"e6fd87b8-c3d6-459b-aa55-593b6b8e8862"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ Advanced RAG Techniques Assignment - Final Test\n","============================================================\n","\n","ğŸ“Š Component Status:\n","   âœ… Basic Index\n","   âœ… Similarity Filter\n","   âœ… TreeSummarize\n","   âœ… Structured Output\n","   âœ… Advanced Pipeline\n","\n","ğŸ” Creating basic query engine for comparison...\n","\n","============================================================\n","ğŸ†š COMPARISON: Basic vs Advanced RAG\n","============================================================\n","\n","ğŸ“‹ Test Query 1: 'What are the key capabilities of AI agents?'\n","--------------------------------------------------\n","ğŸ”¹ Basic RAG:\n","   (Standard vector search + simple response)\n","\n","ğŸ”¸ Advanced RAG:\n","   (Filtered + TreeSummarize + Structured output)\n","\n","ğŸ“‹ Test Query 2: 'How do you evaluate agent performance metrics?'\n","--------------------------------------------------\n","ğŸ”¹ Basic RAG:\n","   (Standard vector search + simple response)\n","\n","ğŸ”¸ Advanced RAG:\n","   (Filtered + TreeSummarize + Structured output)\n","\n","ğŸ“‹ Test Query 3: 'Explain the benefits and challenges of multimodal AI systems'\n","--------------------------------------------------\n","ğŸ”¹ Basic RAG:\n","   (Standard vector search + simple response)\n","\n","ğŸ”¸ Advanced RAG:\n","   (Filtered + TreeSummarize + Structured output)\n","\n","============================================================\n","ğŸ¯ Assignment Status:\n","   Completed: 5/5 components\n","\n","ğŸ‰ Congratulations! You've mastered Advanced RAG Techniques!\n","   âœ… Node postprocessors for result filtering\n","   âœ… Response synthesizers for better answers\n","   âœ… Structured outputs for reliable data\n","   âœ… Advanced pipelines combining all techniques\n","\n","ğŸš€ You're ready for production RAG systems!\n","\n","ğŸ’¡ Key learnings:\n","   - Postprocessors improve result relevance and precision\n","   - Different synthesizers work better for different query types\n","   - Structured outputs enable reliable system integration\n","   - Advanced techniques can be combined for production systems\n"]}],"source":["# Final comparison: Basic vs Advanced RAG\n","print(\"ğŸš€ Advanced RAG Techniques Assignment - Final Test\")\n","print(\"=\" * 60)\n","\n","# Test queries for comparison\n","test_queries = [\n","    \"What are the key capabilities of AI agents?\",\n","    \"How do you evaluate agent performance metrics?\",\n","    \"Explain the benefits and challenges of multimodal AI systems\"\n","]\n","\n","# Check if all components were created\n","components_status = {\n","    \"Basic Index\": index is not None,\n","    \"Similarity Filter\": 'filtered_engine' in locals() and filtered_engine is not None,\n","    \"TreeSummarize\": 'tree_engine' in locals() and tree_engine is not None,\n","    \"Structured Output\": 'structured_program' in locals() and structured_program is not None,\n","    \"Advanced Pipeline\": 'advanced_pipeline' in locals() and advanced_pipeline is not None\n","}\n","\n","print(\"\\nğŸ“Š Component Status:\")\n","for component, status in components_status.items():\n","    status_icon = \"âœ…\" if status else \"âŒ\"\n","    print(f\"   {status_icon} {component}\")\n","\n","# Create basic query engine for comparison\n","if index:\n","    print(\"\\nğŸ” Creating basic query engine for comparison...\")\n","    basic_engine = index.as_query_engine(similarity_top_k=5)\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"ğŸ†š COMPARISON: Basic vs Advanced RAG\")\n","    print(\"=\" * 60)\n","\n","    for i, query in enumerate(test_queries, 1):\n","        print(f\"\\nğŸ“‹ Test Query {i}: '{query}'\")\n","        print(\"-\" * 50)\n","\n","        # Basic RAG\n","        print(\"ğŸ”¹ Basic RAG:\")\n","        if basic_engine:\n","            # Uncomment when testing:\n","            # basic_response = basic_engine.query(query)\n","            # print(f\"   Response: {str(basic_response)[:200]}...\")\n","            print(\"   (Standard vector search + simple response)\")\n","\n","        # Advanced RAG (if implemented)\n","        print(\"\\nğŸ”¸ Advanced RAG:\")\n","        if components_status[\"Advanced Pipeline\"]:\n","            # Uncomment when testing:\n","            # advanced_response = advanced_pipeline.query(query)\n","            # print(f\"   Response: {advanced_response}\")\n","            print(\"   (Filtered + TreeSummarize + Structured output)\")\n","        else:\n","            print(\"   Complete the advanced pipeline function to test\")\n","\n","# Final status\n","print(\"\\n\" + \"=\" * 60)\n","print(\"ğŸ¯ Assignment Status:\")\n","completed_count = sum(components_status.values())\n","total_count = len(components_status)\n","\n","print(f\"   Completed: {completed_count}/{total_count} components\")\n","\n","if completed_count == total_count:\n","    print(\"\\nğŸ‰ Congratulations! You've mastered Advanced RAG Techniques!\")\n","    print(\"   âœ… Node postprocessors for result filtering\")\n","    print(\"   âœ… Response synthesizers for better answers\")\n","    print(\"   âœ… Structured outputs for reliable data\")\n","    print(\"   âœ… Advanced pipelines combining all techniques\")\n","    print(\"\\nğŸš€ You're ready for production RAG systems!\")\n","else:\n","    missing = total_count - completed_count\n","    print(f\"\\nğŸ“ Complete {missing} more components to finish the assignment:\")\n","    for component, status in components_status.items():\n","        if not status:\n","            print(f\"   - {component}\")\n","\n","print(\"\\nğŸ’¡ Key learnings:\")\n","print(\"   - Postprocessors improve result relevance and precision\")\n","print(\"   - Different synthesizers work better for different query types\")\n","print(\"   - Structured outputs enable reliable system integration\")\n","print(\"   - Advanced techniques can be combined for production systems\")\n"]}],"metadata":{"kernelspec":{"display_name":"accelerator","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13"},"colab":{"provenance":[{"file_id":"https://github.com/eng-accelerator/ai-accelerator-C4/blob/main/Day_7/assignments/assignment_2_advanced_rag.ipynb","timestamp":1770543178632}]},"widgets":{"application/vnd.jupyter.widget-state+json":{"8f704a53123a4085b27b7a30369f9643":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8c21b60c16ce4077b30f84e316086015","IPY_MODEL_c32589679603455daf0e87f727b17b06","IPY_MODEL_223b7615f2434098aa5f30c6e8ee46e6"],"layout":"IPY_MODEL_3a7e9f3ed32e49b9a8352d838b231d32"}},"8c21b60c16ce4077b30f84e316086015":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c1c0764df7a43359e771bda8de6547c","placeholder":"â€‹","style":"IPY_MODEL_f5f8596d3c574a298b94a0937523410d","value":"Loadingâ€‡weights:â€‡100%"}},"c32589679603455daf0e87f727b17b06":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8afb3a752fe34611b7323a9c28609fda","max":199,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e6db573d7d63413fa394ebbf7558f0d9","value":199}},"223b7615f2434098aa5f30c6e8ee46e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83bc837425bf4c888c3ea8c9999776ea","placeholder":"â€‹","style":"IPY_MODEL_a122930c593448f28edccb357cbb077a","value":"â€‡199/199â€‡[00:00&lt;00:00,â€‡569.20it/s,â€‡Materializingâ€‡param=pooler.dense.weight]"}},"3a7e9f3ed32e49b9a8352d838b231d32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c1c0764df7a43359e771bda8de6547c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5f8596d3c574a298b94a0937523410d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8afb3a752fe34611b7323a9c28609fda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6db573d7d63413fa394ebbf7558f0d9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"83bc837425bf4c888c3ea8c9999776ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a122930c593448f28edccb357cbb077a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"019b60bb634341a391050c111d3ecfd8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_456cbc890e5a42b19d7ba209d02e2efb","IPY_MODEL_e95be18aed3c4361a58fe84559f2796a","IPY_MODEL_f17aa823794d49dba9068456e1a3cff3"],"layout":"IPY_MODEL_830988ccbaae4c2782324bf2ea5727e5"}},"456cbc890e5a42b19d7ba209d02e2efb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_101102c483fc420d833a47e1a93edac4","placeholder":"â€‹","style":"IPY_MODEL_4df1f1c7f77d4027a442cffa39d0a7a6","value":"Parsingâ€‡nodes:â€‡100%"}},"e95be18aed3c4361a58fe84559f2796a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ebbd28fa2b349cbb6c3605037cdefb6","max":42,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bcb86af265ee4da18c429846171b26be","value":42}},"f17aa823794d49dba9068456e1a3cff3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_afb7f3a4967e4b9299387651b3dd39e8","placeholder":"â€‹","style":"IPY_MODEL_32dd122e559740969ed5433b1a142b20","value":"â€‡42/42â€‡[00:00&lt;00:00,â€‡210.32it/s]"}},"830988ccbaae4c2782324bf2ea5727e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"101102c483fc420d833a47e1a93edac4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4df1f1c7f77d4027a442cffa39d0a7a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ebbd28fa2b349cbb6c3605037cdefb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcb86af265ee4da18c429846171b26be":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"afb7f3a4967e4b9299387651b3dd39e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32dd122e559740969ed5433b1a142b20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf82b8506fe646c496b9637f4525f9fc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d0d75a276894f1893e8fd9f1250093f","IPY_MODEL_1d7fd944f9f443dba162bd92b1628239","IPY_MODEL_257fc9e289c64a2baa164d7f3a5ab87b"],"layout":"IPY_MODEL_35137caaf2fe452fa67c01cd26f6dbe7"}},"9d0d75a276894f1893e8fd9f1250093f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6dcf66104c64b9d8836f48aefee7072","placeholder":"â€‹","style":"IPY_MODEL_335c0500a6e249d095f7e32cadd1e42b","value":"Generatingâ€‡embeddings:â€‡100%"}},"1d7fd944f9f443dba162bd92b1628239":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_02c44a29522b4722adfe7d4338d25865","max":90,"min":0,"orientation":"horizontal","style":"IPY_MODEL_24b30542f03947c28df05c4415fe9ea6","value":90}},"257fc9e289c64a2baa164d7f3a5ab87b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f5f8fc7f0cb4b0597911cfa6e63f84f","placeholder":"â€‹","style":"IPY_MODEL_a42c7d71a2d94cac87f9d198fb92664f","value":"â€‡90/90â€‡[00:48&lt;00:00,â€‡â€‡1.80it/s]"}},"35137caaf2fe452fa67c01cd26f6dbe7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6dcf66104c64b9d8836f48aefee7072":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"335c0500a6e249d095f7e32cadd1e42b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02c44a29522b4722adfe7d4338d25865":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24b30542f03947c28df05c4415fe9ea6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2f5f8fc7f0cb4b0597911cfa6e63f84f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a42c7d71a2d94cac87f9d198fb92664f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}