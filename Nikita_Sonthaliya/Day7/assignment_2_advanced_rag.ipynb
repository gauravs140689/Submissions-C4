{"cells":[{"cell_type":"markdown","metadata":{"id":"02IM2NVg63uU"},"source":["# Assignment 2: Advanced RAG Techniques\n","## Day 6 Session 2 - Advanced RAG Fundamentals\n","\n","**OBJECTIVE:** Implement advanced RAG techniques including postprocessors, response synthesizers, and structured outputs.\n","\n","**LEARNING GOALS:**\n","- Understand and implement node postprocessors for filtering and reranking\n","- Learn different response synthesis strategies (TreeSummarize, Refine)\n","- Create structured outputs using Pydantic models\n","- Build advanced retrieval pipelines with multiple processing stages\n","\n","**DATASET:** Use the same data folder as Assignment 1 (`Day_6/session_2/data/`)\n","\n","**PREREQUISITES:** Complete Assignment 1 first\n","\n","**INSTRUCTIONS:**\n","1. Complete each function by replacing the TODO comments with actual implementation\n","2. Run each cell after completing the function to test it\n","3. The answers can be found in the `03_advanced_rag_techniques.ipynb` notebook\n","4. Each technique builds on the previous one\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lPyeMk_i9Cfk","executionInfo":{"status":"ok","timestamp":1770539774812,"user_tz":-120,"elapsed":26932,"user":{"displayName":"nikita","userId":"03096943508975047854"}},"outputId":"9e8eff98-0bc3-4ad6-cb00-a7c0ad057fd8"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# install dependencies\n","!pip install -q -r \"/content/drive/MyDrive/Colab Notebooks/Day 7/requirements.txt\"\n","# \"/content/requirements.txt\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GRreoue5ZY_C","executionInfo":{"status":"ok","timestamp":1770539878983,"user_tz":-120,"elapsed":57660,"user":{"displayName":"nikita","userId":"03096943508975047854"}},"outputId":"6ed6a088-871e-46c6-9efa-06472326a58f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m182.0/182.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: huggingface-hub 1.3.7 does not provide the extra 'inference'\u001b[0m\u001b[33m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.0/278.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.8/96.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Nc0qhzTA63uW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770540273724,"user_tz":-120,"elapsed":25897,"user":{"displayName":"nikita","userId":"03096943508975047854"}},"outputId":"d09965bf-5609-41ca-dcee-82ffca1c83d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Advanced RAG libraries imported successfully!\n"]}],"source":["# Import required libraries for advanced RAG\n","import os\n","from pathlib import Path\n","from typing import Dict, List, Optional, Any\n","from pydantic import BaseModel, Field\n","\n","# Core LlamaIndex components\n","from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\n","from llama_index.core.query_engine import RetrieverQueryEngine\n","from llama_index.core.retrievers import VectorIndexRetriever\n","\n","# Vector store\n","from llama_index.vector_stores.lancedb import LanceDBVectorStore\n","\n","# Embeddings and LLM\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.llms.openrouter import OpenRouter\n","\n","# Advanced RAG components (we'll use these in the assignments)\n","from llama_index.core.postprocessor import SimilarityPostprocessor\n","from llama_index.core.response_synthesizers import TreeSummarize, Refine, CompactAndRefine\n","from llama_index.core.output_parsers import PydanticOutputParser\n","\n","print(\"âœ… Advanced RAG libraries imported successfully!\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"0ZQ1KsmL63uX","colab":{"base_uri":"https://localhost:8080/","height":257,"referenced_widgets":["78a8fd2c6584444a8766c2f56ac27f58","6c12d3685b574eec9c7aa79b0becaf47","737b2652ac0343a0afe0efcbbcb2639b","832a664ed17d435c82ef221b9c034de1","62a00e860a4f42ddb03e3c3303019306","efbb57d6497a40ea8e76b5cce649b475","ba24365ce4c949aa91fec4160ec3782c","35c6f42b5ec7406fa5adbd304261beff","fcf6fd94c6264f2184669241c2afe0c8","1b21b1374cd540b5b65af250c2d33867","6b63e2f5b1db4f02bc35d5c5d8b6bd16"]},"executionInfo":{"status":"ok","timestamp":1770540542639,"user_tz":-120,"elapsed":4753,"user":{"displayName":"nikita","userId":"03096943508975047854"}},"outputId":"7baa4e77-360e-4e35-b915-c147c44210ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… OPENROUTER_API_KEY found - full advanced RAG functionality available\n"]},{"output_type":"display_data","data":{"text/plain":["Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a8fd2c6584444a8766c2f56ac27f58"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["BertModel LOAD REPORT from: BAAI/bge-small-en-v1.5\n","Key                     | Status     |  | \n","------------------------+------------+--+-\n","embeddings.position_ids | UNEXPECTED |  | \n","\n","Notes:\n","- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Advanced RAG settings configured\n","   - Chunk size: 512 (optimized for precision)\n","   - Using local embeddings for cost efficiency\n","   - OpenRouter LLM ready for response synthesis\n"]}],"source":["# Configure Advanced RAG Settings (Using OpenRouter)\n","def setup_advanced_rag_settings():\n","    \"\"\"\n","    Configure LlamaIndex with optimized settings for advanced RAG.\n","    Uses local embeddings and OpenRouter for LLM operations.\n","    \"\"\"\n","    # Check for OpenRouter API key\n","    from google.colab import userdata\n","    api_key = userdata.get('OPENROUTER_API_KEY')\n","    #api_key = os.getenv(\"OPENROUTER_API_KEY\")\n","    if not api_key:\n","        print(\"âš ï¸  OPENROUTER_API_KEY not found - LLM operations will be limited\")\n","        print(\"   You can still complete postprocessor and retrieval exercises\")\n","    else:\n","        print(\"âœ… OPENROUTER_API_KEY found - full advanced RAG functionality available\")\n","\n","        # Configure OpenRouter LLM\n","        Settings.llm = OpenRouter(\n","            api_key=api_key,\n","            model=\"gpt-4o\",\n","            temperature=0.1  # Lower temperature for more consistent responses\n","        )\n","\n","    # Configure local embeddings (no API key required)\n","    Settings.embed_model = HuggingFaceEmbedding(\n","        model_name=\"BAAI/bge-small-en-v1.5\",\n","        trust_remote_code=True\n","    )\n","\n","    # Advanced RAG configuration\n","    Settings.chunk_size = 512  # Smaller chunks for better precision\n","    Settings.chunk_overlap = 50\n","\n","    print(\"âœ… Advanced RAG settings configured\")\n","    print(\"   - Chunk size: 512 (optimized for precision)\")\n","    print(\"   - Using local embeddings for cost efficiency\")\n","    print(\"   - OpenRouter LLM ready for response synthesis\")\n","\n","# Setup the configuration\n","setup_advanced_rag_settings()\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"XI5ENbpD63uX","colab":{"base_uri":"https://localhost:8080/","height":289,"referenced_widgets":["012e13d22b4e413693a52f3dd82736a3","e1d351c6f0b3407e88b47be8a1f0d328","23d5c6a9f6074bf780ff0de017f11321","a39073d5b3764c7e9ce2a5c0153058f1","ed135c59e1934204a8124132620b89c1","4eb5fbe38b9b4b50b0d1ea32414e89a2","ed90a29697a04c81a3f86325a44bd6c1","f6bf9cb5d1a142f3a09575a94be404ef","26cbfb45f1cf45c78dca3f9116db9135","01b314bc0eee4f6cb0d5d6eab6160268","1b9cde2cea01489f899a881ebef2c651","935b0294099a4c489ed92ea3c26db465","a7cc354761794770b9b3d0fcb62143c1","38592405b1c1480d94b65724adf7a84b","edddd5b7450a4055841bcb0b165305f9","450f74480ee04a61b629063299f26a63","e29ee8b627fe4b33a69725cc0e5d29b2","c0807be85b3843c2b61eb4a49d44973f","882fa627647b466f8a9d0cfc57a974aa","d322e19546784192b68bdc51abd40ffd","412a7fa6c8fb4995b09639915ad9e888","0f274748cf374760934151ef81931d81"]},"executionInfo":{"status":"ok","timestamp":1770540709067,"user_tz":-120,"elapsed":101824,"user":{"displayName":"nikita","userId":"03096943508975047854"}},"outputId":"73d61d24-ee35-4bbd-ceb2-dd0a360d82b0"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:llama_index.vector_stores.lancedb.base:Table documents doesn't exist yet. Please add some data to create it.\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ“ Setting up basic index for advanced RAG...\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139M/139M [00:01<00:00, 114MiB/s]\n","/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n","/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n","  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"]},{"output_type":"display_data","data":{"text/plain":["Parsing nodes:   0%|          | 0/42 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"012e13d22b4e413693a52f3dd82736a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating embeddings:   0%|          | 0/92 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"935b0294099a4c489ed92ea3c26db465"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ… Basic index created with 42 documents\n","   Ready for advanced RAG techniques!\n","ğŸš€ Ready to implement advanced RAG techniques!\n"]}],"source":["# Setup: Create index from Assignment 1 (reuse the basic functionality)\n","def setup_basic_index(data_folder: str = \"/content/drive/MyDrive/Colab Notebooks/Atlas_Sample_Data/RAG Multi Model\", force_rebuild: bool = False):\n","    \"\"\"\n","    Create a basic vector index that we'll enhance with advanced techniques.\n","    This reuses the concepts from Assignment 1.\n","    \"\"\"\n","    # Create vector store\n","    vector_store = LanceDBVectorStore(\n","        uri=\"./advanced_rag_vectordb\",\n","        table_name=\"documents\"\n","    )\n","\n","    # Load documents\n","    if not Path(data_folder).exists():\n","        print(f\"âŒ Data folder not found: {data_folder}\")\n","        return None\n","\n","    reader = SimpleDirectoryReader(input_dir=data_folder, recursive=True)\n","    documents = reader.load_data()\n","\n","    # Create storage context and index\n","    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n","    index = VectorStoreIndex.from_documents(\n","        documents,\n","        storage_context=storage_context,\n","        show_progress=True\n","    )\n","\n","    print(f\"âœ… Basic index created with {len(documents)} documents\")\n","    print(\"   Ready for advanced RAG techniques!\")\n","    return index\n","\n","# Create the basic index\n","print(\"ğŸ“ Setting up basic index for advanced RAG...\")\n","index = setup_basic_index()\n","\n","if index:\n","    print(\"ğŸš€ Ready to implement advanced RAG techniques!\")\n","else:\n","    print(\"âŒ Failed to create index - check data folder path\")\n"]},{"cell_type":"markdown","metadata":{"id":"eQVRpOuk63uY"},"source":["## 1. Node Postprocessors - Similarity Filtering\n","\n","**Concept:** Postprocessors refine retrieval results after the initial vector search. The `SimilarityPostprocessor` filters out chunks that fall below a relevance threshold.\n","\n","**Why it matters:** Raw vector search often returns some irrelevant results. Filtering improves precision and response quality.\n","\n","Complete the function below to create a query engine with similarity filtering.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"OAyqtT7O63uY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770540862351,"user_tz":-120,"elapsed":7452,"user":{"displayName":"nikita","userId":"03096943508975047854"}},"outputId":"df684010-1d0d-4ff0-a5d8-f893d0e207dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Query engine with similarity filtering created\n","\n","ğŸ” Testing query: 'What are the benefits of AI agents?'\n","ğŸ“ Response: AI agents offer several benefits, including the ability to effectively reason and plan, which allows them to operate in complex environments and make autonomous decisions. They can handle complex tasks by using various tools, enabling interaction with external data sources and APIs. Additionally, AI agents can iteratively pursue goals, modifying their strategies based on new information and feedback. Multi-agent systems, in particular, can distribute tasks and enhance collaboration among agents with different roles, thereby improving performance on difficult tasks. These capabilities make AI agents crucial for tackling real-world challenges and achieving complex objectives.\n"]}],"source":["def create_query_engine_with_similarity_filter(index, similarity_cutoff: float = 0.3, top_k: int = 10):\n","    \"\"\"\n","    Create a query engine that filters results based on similarity scores.\n","\n","    TODO: Complete this function to create a query engine with similarity postprocessing.\n","    HINT: Use index.as_query_engine() with node_postprocessors parameter containing SimilarityPostprocessor\n","\n","    Args:\n","        index: Vector index to query\n","        similarity_cutoff: Minimum similarity score (0.0 to 1.0)\n","        top_k: Number of initial results to retrieve before filtering\n","\n","    Returns:\n","        Query engine with similarity filtering\n","    \"\"\"\n","    # TODO: Create similarity postprocessor with the cutoff threshold\n","    similarity_processor = SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)\n","\n","    # TODO: Create query engine with similarity filtering\n","    query_engine =  index.as_query_engine(similarity_top_k=top_k, node_postprocessors=[similarity_processor])\n","\n","    return query_engine\n","\n","    # PLACEHOLDER - Replace with actual implementation\n","    print(f\"TODO: Create query engine with similarity cutoff {similarity_cutoff}\")\n","    return None\n","\n","# Test the function\n","if index:\n","    filtered_engine = create_query_engine_with_similarity_filter(index, similarity_cutoff=0.3)\n","\n","    if filtered_engine:\n","        print(\"âœ… Query engine with similarity filtering created\")\n","\n","        # Test query\n","        test_query = \"What are the benefits of AI agents?\"\n","        print(f\"\\nğŸ” Testing query: '{test_query}'\")\n","\n","        # Uncomment when implemented:\n","        response = filtered_engine.query(test_query)\n","        print(f\"ğŸ“ Response: {response}\")\n","        # print(\"   (Complete the function above to test the response)\")\n","    else:\n","        print(\"âŒ Failed to create filtered query engine\")\n","else:\n","    print(\"âŒ No index available - run previous cells first\")\n"]},{"cell_type":"markdown","metadata":{"id":"VRdfQuy363uZ"},"source":["## 2. Response Synthesizers - TreeSummarize\n","\n","**Concept:** Response synthesizers control how retrieved information becomes final answers. `TreeSummarize` builds responses hierarchically, ideal for complex analytical questions.\n","\n","**Why it matters:** Different synthesis strategies work better for different query types. TreeSummarize excels at comprehensive analysis and long-form responses.\n","\n","Complete the function below to create a query engine with TreeSummarize response synthesis.\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"48leCpvR63uZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770541010782,"user_tz":-120,"elapsed":3455,"user":{"displayName":"nikita","userId":"03096943508975047854"}},"outputId":"c25a9dc1-02b8-42d6-f7f6-71cbff49d693"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Query engine with TreeSummarize created\n","\n","ğŸ” Testing analytical query: 'Compare the advantages and disadvantages of different AI agent frameworks'\n","1 text chunks after repacking\n","ğŸ“ TreeSummarize Response:\n","Different AI agent frameworks offer various advantages and disadvantages based on their design and intended use cases.\n","\n","**Advantages:**\n","\n","1. **Autonomous Agents:**\n","   - **AutoGPT**: Known for its high capability in autonomous task execution, making it suitable for complex tasks that require minimal human intervention.\n","   - **BabyAGI**: Offers a simplified approach, which can be beneficial for those looking to implement autonomous AI without extensive complexity.\n","   - **AgentGPT**: Provides a web-based platform, making it accessible and easy to deploy for users who prefer online solutions.\n","\n","2. **Tool-Using Agents:**\n","   - **LangChain**: Offers a comprehensive framework for large language model (LLM) applications, making it versatile for general use cases.\n","   - **LlamaIndex**: Specializes in document understanding and retrieval-augmented generation (RAG), making it ideal for document-based Q&A tasks.\n","   - **Semantic Kernel**: Microsoft's approach focuses on AI orchestration, which can be advantageous for integrating various AI tools and services.\n","\n","3. **Multi-Agent Systems:**\n","   - **CrewAI**: Facilitates collaborative agent teams, which can enhance performance in tasks requiring teamwork and coordination.\n","   - **MetaGPT**: Focuses on multi-agent\n"]}],"source":["def create_query_engine_with_tree_summarize(index, top_k: int = 5):\n","    \"\"\"\n","    Create a query engine that uses TreeSummarize for comprehensive responses.\n","\n","    TODO: Complete this function to create a query engine with TreeSummarize synthesis.\n","    HINT: Create a TreeSummarize instance, then use index.as_query_engine() with response_synthesizer parameter\n","\n","    Args:\n","        index: Vector index to query\n","        top_k: Number of results to retrieve\n","\n","    Returns:\n","        Query engine with TreeSummarize synthesis\n","    \"\"\"\n","    # TODO: Create TreeSummarize response synthesizer\n","    tree_synthesizer = TreeSummarize(verbose=True)\n","\n","    # TODO: Create query engine with the synthesizer\n","    query_engine =  index.as_query_engine(response_synthesizer=tree_synthesizer, similarity_top_k=top_k)\n","\n","    return query_engine\n","\n","    # PLACEHOLDER - Replace with actual implementation\n","    # print(f\"TODO: Create query engine with TreeSummarize synthesis\")\n","    # return None\n","\n","# Test the function\n","if index:\n","    tree_engine = create_query_engine_with_tree_summarize(index)\n","\n","    if tree_engine:\n","        print(\"âœ… Query engine with TreeSummarize created\")\n","\n","        # Test with a complex analytical query\n","        analytical_query = \"Compare the advantages and disadvantages of different AI agent frameworks\"\n","        print(f\"\\nğŸ” Testing analytical query: '{analytical_query}'\")\n","\n","        # Uncomment when implemented:\n","        response = tree_engine.query(analytical_query)\n","        print(f\"ğŸ“ TreeSummarize Response:\\n{response}\")\n","        #print(\"   (Complete the function above to test comprehensive analysis)\")\n","    else:\n","        print(\"âŒ Failed to create TreeSummarize query engine\")\n","else:\n","    print(\"âŒ No index available - run previous cells first\")\n"]},{"cell_type":"markdown","metadata":{"id":"sLMwYqh263uZ"},"source":["## 3. Structured Outputs with Pydantic Models\n","\n","**Concept:** Structured outputs ensure predictable, parseable responses using Pydantic models. This is essential for API endpoints and data pipelines.\n","\n","**Why it matters:** Instead of free-text responses, you get type-safe, validated data structures that applications can reliably process.\n","\n","Complete the function below to create a structured output system for extracting research paper information.\n"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"Ql0l8WDg63ua","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770542476798,"user_tz":-120,"elapsed":2572,"user":{"displayName":"nikita","userId":"03096943508975047854"}},"outputId":"202ca404-9ef1-489e-c192-d9b3823ed046"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Structured output program created\n","\n","ğŸ” Testing structured query: 'Tell me about AI agents and their capabilities'\n","ğŸ“Š Structured Response:\n","title='AI Agents and Their Capabilities' key_points=['Architectures leveraging dynamic teams and intelligent message filtering improve performance.', 'Single and multi-agent patterns excel in complex tasks with reasoning and tool execution.', 'Challenges exist in agent evaluation due to varying benchmarks and potential biases.', 'Vertical and horizontal multi-agent architectures offer different collaboration models.', 'Agents require reasoning, planning, and tool calling to solve real-world problems effectively.'] applications=['Solving complex reasoning tasks', 'Executing tasks with tool integration', 'Collaborative problem-solving in dynamic environments'] summary='AI agents extend language model capabilities to address real-world challenges. They perform well in complex tasks through reasoning, planning, and tool calling. However, challenges in evaluation and biases need to be addressed for reliable agent deployment.'\n","\n","ğŸ’¡ Expected output format:\n","   - title: String\n","   - key_points: List of strings\n","   - applications: List of strings\n","   - summary: String\n"]}],"source":["# First, define the Pydantic models for structured outputs\n","class ResearchPaperInfo(BaseModel):\n","    \"\"\"Structured information about a research paper or AI concept.\"\"\"\n","    title: str = Field(description=\"The main title or concept name\")\n","    key_points: List[str] = Field(description=\"3-5 main points or findings\")\n","    applications: List[str] = Field(description=\"Practical applications or use cases\")\n","    summary: str = Field(description=\"Brief 2-3 sentence summary\")\n","\n","# Import the missing component\n","from llama_index.core.program import LLMTextCompletionProgram\n","\n","def create_structured_output_program(output_model: BaseModel = ResearchPaperInfo):\n","    \"\"\"\n","    Create a structured output program using Pydantic models.\n","\n","    TODO: Complete this function to create a structured output program.\n","    HINT: Use LLMTextCompletionProgram.from_defaults() with PydanticOutputParser and a prompt template\n","\n","    Args:\n","        output_model: Pydantic model class for structured output\n","\n","    Returns:\n","        LLMTextCompletionProgram that returns structured data\n","    \"\"\"\n","    # TODO: Create output parser with the Pydantic model\n","    #output_parser = PydanticOutputParser(pydantic_object=output_model)\n","    output_parser = PydanticOutputParser(output_model)\n","\n","    prompt_template_str = \"\"\"\n","    You are an expert AI research analyst.\n","\n","    Given the following context, extract structured information.\n","\n","    Follow these rules:\n","    - Use only the information from the context.\n","    - If something is missing, infer cautiously.\n","    - Do not add extra fields.\n","    - Do not include explanations.\n","\n","    Context:\n","    {context}\n","\n","    Query:\n","    {query}\n","\n","    Return the output in this exact structure:\n","    - title: String\n","    - key_points: List of strings (3 to 5 items)\n","    - applications: List of strings\n","    - summary: String (2 to 3 sentences)\n","    \"\"\"\n","\n","    # TODO: Create the structured output program\n","    program = LLMTextCompletionProgram.from_defaults(\n","        output_parser=output_parser,\n","        prompt_template_str=prompt_template_str,\n","        verbose=True\n","        )\n","    return program\n","\n","    # PLACEHOLDER - Replace with actual implementation\n","    print(f\"TODO: Create structured output program with {output_model.__name__}\")\n","    return None\n","\n","# Test the function\n","if index:\n","    structured_program = create_structured_output_program(ResearchPaperInfo)\n","\n","    if structured_program:\n","        print(\"âœ… Structured output program created\")\n","\n","        # Test with retrieval and structured extraction\n","        structure_query = \"Tell me about AI agents and their capabilities\"\n","        print(f\"\\nğŸ” Testing structured query: '{structure_query}'\")\n","\n","        # Get context for structured extraction (Uncomment when implemented)\n","        retriever = VectorIndexRetriever(index=index, similarity_top_k=3)\n","        nodes = retriever.retrieve(structure_query)\n","        context = \"\\n\".join([node.text for node in nodes])\n","\n","        # Uncomment when implemented:\n","        response = structured_program(context=context, query=structure_query)\n","        print(f\"ğŸ“Š Structured Response:\\n{response}\")\n","        #print(\"   (Complete the function above to get structured JSON output)\")\n","\n","        print(\"\\nğŸ’¡ Expected output format:\")\n","        print(\"   - title: String\")\n","        print(\"   - key_points: List of strings\")\n","        print(\"   - applications: List of strings\")\n","        print(\"   - summary: String\")\n","    else:\n","        print(\"âŒ Failed to create structured output program\")\n","else:\n","    print(\"âŒ No index available - run previous cells first\")\n"]},{"cell_type":"markdown","metadata":{"id":"Po06cBWA63ua"},"source":["## 4. Advanced Pipeline - Combining All Techniques\n","\n","**Concept:** Combine multiple advanced techniques into a single powerful query engine: similarity filtering + response synthesis + structured output.\n","\n","**Why it matters:** Production RAG systems often need multiple techniques working together for optimal results.\n","\n","Complete the function below to create a comprehensive advanced RAG pipeline.\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"n6tjyNFc63ua","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770542666578,"user_tz":-120,"elapsed":9506,"user":{"displayName":"nikita","userId":"03096943508975047854"}},"outputId":"c61a7055-a00f-4ed7-d125-25cd0516d2a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Advanced RAG pipeline created successfully!\n","   ğŸ”§ Similarity filtering: âœ…\n","   ğŸŒ³ TreeSummarize synthesis: âœ…\n","\n","ğŸ” Testing complex query: 'Analyze the current state and future potential of AI agent technologies'\n","2 text chunks after repacking\n","1 text chunks after repacking\n","ğŸš€ Advanced RAG Response:\n","AI agent technologies have made significant strides, particularly in achieving complex goals through advanced reasoning, planning, and tool execution. Current implementations, whether single-agent or multi-agent, excel in handling complex tasks due to their dynamic and autonomous architectures. However, they face challenges such as the need for comprehensive benchmarks, real-world applicability, and mitigating biases in language models.\n","\n","The future potential of AI agent technologies is promising, with a focus on overcoming these limitations. Enhancing evaluation methods, ensuring system reliability, and addressing biases are crucial steps toward developing more robust AI systems. The integration of AI agents into various domains, such as finance, suggests a transformative impact on industry practices through automation and intelligent decision-making.\n","\n","In finance, AI agents are being explored for their ability to manage complex scenarios and make real-time decisions in dynamic markets. These technologies are moving from experimental stages to production-scale solutions, addressing challenges like scalability, integration with legacy systems, and operational risk management. Future research is expected to delve into applications in wealth management, emphasizing standardization and best practices to ensure reliability and trustworthiness. Collaboration among academia, industry, and open-source communities will be vital in realizing the full potential of agentic AI.\n","\n","ğŸ¯ This should provide:\n","   - Filtered relevant results only\n","   - Comprehensive analytical response\n","   - Combined postprocessing and synthesis\n"]}],"source":["def create_advanced_rag_pipeline(index, similarity_cutoff: float = 0.3, top_k: int = 10):\n","    \"\"\"\n","    Create a comprehensive advanced RAG pipeline combining multiple techniques.\n","\n","    TODO: Complete this function to create the ultimate advanced RAG query engine.\n","    HINT: Combine SimilarityPostprocessor + TreeSummarize using index.as_query_engine()\n","\n","    Args:\n","        index: Vector index to query\n","        similarity_cutoff: Minimum similarity score for filtering\n","        top_k: Number of initial results to retrieve\n","\n","    Returns:\n","        Advanced query engine with filtering and synthesis combined\n","    \"\"\"\n","    # TODO: Create similarity postprocessor\n","    similarity_processor =  SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)\n","\n","    # TODO: Create TreeSummarize for comprehensive responses\n","    tree_synthesizer =  TreeSummarize(verbose=True)\n","\n","    # TODO: Create the comprehensive query engine combining both techniques\n","    advanced_engine =  index.as_query_engine(similarity_top_k=top_k, node_postprocessors=[similarity_processor], response_synthesizer=tree_synthesizer)\n","\n","    return advanced_engine\n","\n","    # PLACEHOLDER - Replace with actual implementation\n","    #print(f\"TODO: Create advanced RAG pipeline with all techniques\")\n","    #return None\n","\n","# Test the comprehensive pipeline\n","if index:\n","    advanced_pipeline = create_advanced_rag_pipeline(index)\n","\n","    if advanced_pipeline:\n","        print(\"âœ… Advanced RAG pipeline created successfully!\")\n","        print(\"   ğŸ”§ Similarity filtering: âœ…\")\n","        print(\"   ğŸŒ³ TreeSummarize synthesis: âœ…\")\n","\n","        # Test with complex query\n","        complex_query = \"Analyze the current state and future potential of AI agent technologies\"\n","        print(f\"\\nğŸ” Testing complex query: '{complex_query}'\")\n","\n","        # Uncomment when implemented:\n","        response = advanced_pipeline.query(complex_query)\n","        print(f\"ğŸš€ Advanced RAG Response:\\n{response}\")\n","        # print(\"   (Complete the function above to test the full pipeline)\")\n","\n","        print(\"\\nğŸ¯ This should provide:\")\n","        print(\"   - Filtered relevant results only\")\n","        print(\"   - Comprehensive analytical response\")\n","        print(\"   - Combined postprocessing and synthesis\")\n","    else:\n","        print(\"âŒ Failed to create advanced RAG pipeline\")\n","else:\n","    print(\"âŒ No index available - run previous cells first\")\n"]},{"cell_type":"markdown","metadata":{"id":"CiX_qc4e63ub"},"source":["## 5. Final Test - Compare Basic vs Advanced RAG\n","\n","Once you've completed all the functions above, run this cell to compare basic RAG with your advanced techniques.\n"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"apcnHgXs63ub","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1770542786033,"user_tz":-120,"elapsed":27936,"user":{"displayName":"nikita","userId":"03096943508975047854"}},"outputId":"17cf9943-d4a5-4780-d438-3c8192134a31"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸš€ Advanced RAG Techniques Assignment - Final Test\n","============================================================\n","\n","ğŸ“Š Component Status:\n","   âœ… Basic Index\n","   âœ… Similarity Filter\n","   âœ… TreeSummarize\n","   âœ… Structured Output\n","   âœ… Advanced Pipeline\n","\n","ğŸ” Creating basic query engine for comparison...\n","\n","============================================================\n","ğŸ†š COMPARISON: Basic vs Advanced RAG\n","============================================================\n","\n","ğŸ“‹ Test Query 1: 'What are the key capabilities of AI agents?'\n","--------------------------------------------------\n","ğŸ”¹ Basic RAG:\n","   Response: AI agents require several key capabilities to effectively solve real-world problems. These include the ability to reason and plan, which allows them to make autonomous decisions and adjust plans based...\n","\n","ğŸ”¸ Advanced RAG:\n","2 text chunks after repacking\n","1 text chunks after repacking\n","   Response: AI agents have several key capabilities, including reasoning, planning, and tool calling. Reasoning enables them to make decisions, solve problems, and adapt to new information. Planning involves breaking down tasks into sub-tasks, selecting optimal plans, and refining them based on feedback. Tool calling allows them to interact with external data sources and APIs, enhancing their problem-solving abilities. Additionally, they can act autonomously, leverage Large Language Models for sophisticated interactions, and integrate with tools and plugins to perform general-purpose tasks. These capabilities enable them to handle complex environments and novel tasks effectively.\n","\n","ğŸ“‹ Test Query 2: 'How do you evaluate agent performance metrics?'\n","--------------------------------------------------\n","ğŸ”¹ Basic RAG:\n","   Response: Evaluating agent performance metrics involves using both objective and subjective measures. Objective metrics include success rate, output similarity to human responses, and overall efficiency. These ...\n","\n","ğŸ”¸ Advanced RAG:\n","2 text chunks after repacking\n","1 text chunks after repacking\n","   Response: Evaluating agent performance metrics involves a combination of objective and subjective measures. Objective metrics include assessing the success rate, output similarity to human responses, and overall efficiency, which help determine the reliability and accuracy of the agent. Subjective measures require human evaluation and focus on aspects like the efficiency of tool use, reliability, and robustness of planning. Specific benchmarks, such as AgentBench and SmartPlay, provide structured environments for evaluating reasoning, planning, and tool usage. Real-world benchmarks like WildBench and SWE-bench assess performance using real-world data across various tasks or specific domains. Additionally, factors such as the agent's flexibility, robustness, user feedback, and satisfaction are considered, along with specific metrics like hallucination rates or error rates, to understand the reliability and usability of the agent's outputs. Regular monitoring and evaluation are recommended to ensure effective performance in designated roles.\n","\n","ğŸ“‹ Test Query 3: 'Explain the benefits and challenges of multimodal AI systems'\n","--------------------------------------------------\n","ğŸ”¹ Basic RAG:\n","   Response: Multimodal AI systems offer several benefits, including the ability to process and integrate information from multiple sources, such as text, images, and audio. This capability enhances the system's u...\n","\n","ğŸ”¸ Advanced RAG:\n","2 text chunks after repacking\n","1 text chunks after repacking\n","   Response: Multimodal AI systems offer several benefits, including the ability to process and integrate information from various data types like text, images, and audio. This capability enhances the system's understanding and decision-making, leading to more accurate and contextually aware outputs. Additionally, these systems can improve user interaction by providing more natural and intuitive interfaces, accommodating different user preferences and contexts.\n","\n","However, multimodal AI systems also face challenges. One major challenge is the complexity involved in effectively integrating and synchronizing different data modalities, which requires sophisticated algorithms and models. Another challenge is the increased computational resources needed to process and analyze multimodal data, which can be more demanding than unimodal systems. Ensuring the reliability and robustness of these systems across various applications and environments is also a critical concern.\n","\n","============================================================\n","ğŸ¯ Assignment Status:\n","   Completed: 5/5 components\n","\n","ğŸ‰ Congratulations! You've mastered Advanced RAG Techniques!\n","   âœ… Node postprocessors for result filtering\n","   âœ… Response synthesizers for better answers\n","   âœ… Structured outputs for reliable data\n","   âœ… Advanced pipelines combining all techniques\n","\n","ğŸš€ You're ready for production RAG systems!\n","\n","ğŸ’¡ Key learnings:\n","   - Postprocessors improve result relevance and precision\n","   - Different synthesizers work better for different query types\n","   - Structured outputs enable reliable system integration\n","   - Advanced techniques can be combined for production systems\n"]}],"source":["# Final comparison: Basic vs Advanced RAG\n","print(\"ğŸš€ Advanced RAG Techniques Assignment - Final Test\")\n","print(\"=\" * 60)\n","\n","# Test queries for comparison\n","test_queries = [\n","    \"What are the key capabilities of AI agents?\",\n","    \"How do you evaluate agent performance metrics?\",\n","    \"Explain the benefits and challenges of multimodal AI systems\"\n","]\n","\n","# Check if all components were created\n","components_status = {\n","    \"Basic Index\": index is not None,\n","    \"Similarity Filter\": 'filtered_engine' in locals() and filtered_engine is not None,\n","    \"TreeSummarize\": 'tree_engine' in locals() and tree_engine is not None,\n","    \"Structured Output\": 'structured_program' in locals() and structured_program is not None,\n","    \"Advanced Pipeline\": 'advanced_pipeline' in locals() and advanced_pipeline is not None\n","}\n","\n","print(\"\\nğŸ“Š Component Status:\")\n","for component, status in components_status.items():\n","    status_icon = \"âœ…\" if status else \"âŒ\"\n","    print(f\"   {status_icon} {component}\")\n","\n","# Create basic query engine for comparison\n","if index:\n","    print(\"\\nğŸ” Creating basic query engine for comparison...\")\n","    basic_engine = index.as_query_engine(similarity_top_k=5)\n","\n","    print(\"\\n\" + \"=\" * 60)\n","    print(\"ğŸ†š COMPARISON: Basic vs Advanced RAG\")\n","    print(\"=\" * 60)\n","\n","    for i, query in enumerate(test_queries, 1):\n","        print(f\"\\nğŸ“‹ Test Query {i}: '{query}'\")\n","        print(\"-\" * 50)\n","\n","        # Basic RAG\n","        print(\"ğŸ”¹ Basic RAG:\")\n","        if basic_engine:\n","            # Uncomment when testing:\n","            basic_response = basic_engine.query(query)\n","            print(f\"   Response: {str(basic_response)[:200]}...\")\n","            # print(\"   (Standard vector search + simple response)\")\n","\n","        # Advanced RAG (if implemented)\n","        print(\"\\nğŸ”¸ Advanced RAG:\")\n","        if components_status[\"Advanced Pipeline\"]:\n","            # Uncomment when testing:\n","            advanced_response = advanced_pipeline.query(query)\n","            print(f\"   Response: {advanced_response}\")\n","            # print(\"   (Filtered + TreeSummarize + Structured output)\")\n","        else:\n","            print(\"   Complete the advanced pipeline function to test\")\n","\n","# Final status\n","print(\"\\n\" + \"=\" * 60)\n","print(\"ğŸ¯ Assignment Status:\")\n","completed_count = sum(components_status.values())\n","total_count = len(components_status)\n","\n","print(f\"   Completed: {completed_count}/{total_count} components\")\n","\n","if completed_count == total_count:\n","    print(\"\\nğŸ‰ Congratulations! You've mastered Advanced RAG Techniques!\")\n","    print(\"   âœ… Node postprocessors for result filtering\")\n","    print(\"   âœ… Response synthesizers for better answers\")\n","    print(\"   âœ… Structured outputs for reliable data\")\n","    print(\"   âœ… Advanced pipelines combining all techniques\")\n","    print(\"\\nğŸš€ You're ready for production RAG systems!\")\n","else:\n","    missing = total_count - completed_count\n","    print(f\"\\nğŸ“ Complete {missing} more components to finish the assignment:\")\n","    for component, status in components_status.items():\n","        if not status:\n","            print(f\"   - {component}\")\n","\n","print(\"\\nğŸ’¡ Key learnings:\")\n","print(\"   - Postprocessors improve result relevance and precision\")\n","print(\"   - Different synthesizers work better for different query types\")\n","print(\"   - Structured outputs enable reliable system integration\")\n","print(\"   - Advanced techniques can be combined for production systems\")\n"]}],"metadata":{"kernelspec":{"display_name":"accelerator","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"78a8fd2c6584444a8766c2f56ac27f58":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6c12d3685b574eec9c7aa79b0becaf47","IPY_MODEL_737b2652ac0343a0afe0efcbbcb2639b","IPY_MODEL_832a664ed17d435c82ef221b9c034de1"],"layout":"IPY_MODEL_62a00e860a4f42ddb03e3c3303019306"}},"6c12d3685b574eec9c7aa79b0becaf47":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efbb57d6497a40ea8e76b5cce649b475","placeholder":"â€‹","style":"IPY_MODEL_ba24365ce4c949aa91fec4160ec3782c","value":"Loadingâ€‡weights:â€‡100%"}},"737b2652ac0343a0afe0efcbbcb2639b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_35c6f42b5ec7406fa5adbd304261beff","max":199,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fcf6fd94c6264f2184669241c2afe0c8","value":199}},"832a664ed17d435c82ef221b9c034de1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b21b1374cd540b5b65af250c2d33867","placeholder":"â€‹","style":"IPY_MODEL_6b63e2f5b1db4f02bc35d5c5d8b6bd16","value":"â€‡199/199â€‡[00:00&lt;00:00,â€‡629.57it/s,â€‡Materializingâ€‡param=pooler.dense.weight]"}},"62a00e860a4f42ddb03e3c3303019306":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efbb57d6497a40ea8e76b5cce649b475":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba24365ce4c949aa91fec4160ec3782c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35c6f42b5ec7406fa5adbd304261beff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcf6fd94c6264f2184669241c2afe0c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1b21b1374cd540b5b65af250c2d33867":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b63e2f5b1db4f02bc35d5c5d8b6bd16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"012e13d22b4e413693a52f3dd82736a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1d351c6f0b3407e88b47be8a1f0d328","IPY_MODEL_23d5c6a9f6074bf780ff0de017f11321","IPY_MODEL_a39073d5b3764c7e9ce2a5c0153058f1"],"layout":"IPY_MODEL_ed135c59e1934204a8124132620b89c1"}},"e1d351c6f0b3407e88b47be8a1f0d328":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4eb5fbe38b9b4b50b0d1ea32414e89a2","placeholder":"â€‹","style":"IPY_MODEL_ed90a29697a04c81a3f86325a44bd6c1","value":"Parsingâ€‡nodes:â€‡100%"}},"23d5c6a9f6074bf780ff0de017f11321":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6bf9cb5d1a142f3a09575a94be404ef","max":42,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26cbfb45f1cf45c78dca3f9116db9135","value":42}},"a39073d5b3764c7e9ce2a5c0153058f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_01b314bc0eee4f6cb0d5d6eab6160268","placeholder":"â€‹","style":"IPY_MODEL_1b9cde2cea01489f899a881ebef2c651","value":"â€‡42/42â€‡[00:01&lt;00:00,â€‡â€‡1.67s/it]"}},"ed135c59e1934204a8124132620b89c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4eb5fbe38b9b4b50b0d1ea32414e89a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed90a29697a04c81a3f86325a44bd6c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6bf9cb5d1a142f3a09575a94be404ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26cbfb45f1cf45c78dca3f9116db9135":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"01b314bc0eee4f6cb0d5d6eab6160268":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b9cde2cea01489f899a881ebef2c651":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"935b0294099a4c489ed92ea3c26db465":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7cc354761794770b9b3d0fcb62143c1","IPY_MODEL_38592405b1c1480d94b65724adf7a84b","IPY_MODEL_edddd5b7450a4055841bcb0b165305f9"],"layout":"IPY_MODEL_450f74480ee04a61b629063299f26a63"}},"a7cc354761794770b9b3d0fcb62143c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e29ee8b627fe4b33a69725cc0e5d29b2","placeholder":"â€‹","style":"IPY_MODEL_c0807be85b3843c2b61eb4a49d44973f","value":"Generatingâ€‡embeddings:â€‡100%"}},"38592405b1c1480d94b65724adf7a84b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_882fa627647b466f8a9d0cfc57a974aa","max":92,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d322e19546784192b68bdc51abd40ffd","value":92}},"edddd5b7450a4055841bcb0b165305f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_412a7fa6c8fb4995b09639915ad9e888","placeholder":"â€‹","style":"IPY_MODEL_0f274748cf374760934151ef81931d81","value":"â€‡92/92â€‡[00:40&lt;00:00,â€‡â€‡2.36it/s]"}},"450f74480ee04a61b629063299f26a63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e29ee8b627fe4b33a69725cc0e5d29b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0807be85b3843c2b61eb4a49d44973f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"882fa627647b466f8a9d0cfc57a974aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d322e19546784192b68bdc51abd40ffd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"412a7fa6c8fb4995b09639915ad9e888":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f274748cf374760934151ef81931d81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}